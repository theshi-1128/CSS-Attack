认识图计算和图数据库

什么是图

图（Graph）是一种非常直观表达事物及其关联关系的数据结构，基本元素是“点”和“边”，点表示一个事物，边就表示他们之间具有一定关系

比如下面这张图，它的点有公司、员工、项目，边即他们之间的关系——公司和员工之间是雇佣关系、员工和员工之间可以有好友关系、项目和员工之间也可以有参与关系。也就是说我们可以用图的方式来把事物和它们的关系抽象出来。

这是一张结构比较简单的图。随着点和边不断增加，图数据越来越多越来越复杂，逐渐形成更丰富的网状结构。比如一些金融交易图，它的规模可能会非常大，超过 10 亿个点，有千亿甚至万亿边。可以想象，要真正处理这些图还是很有挑战的。

为什么需要图计算

把计算机想象成大脑，我们要解决两个关键问题：数据的存储和分析。

传统的数据存储采用关系型数据库，其结构是“表结构”（想象一下 Excel 表格）。比如一家银行的客户转账信息可能包含交易人员、交易金额、交易时间，记录在一张表格里。假如交易人 A 和 B 之间有直接转账关系，这种直接关系用关系型数据库是不难发现的。但对于非直接关系，关系型数据库就较难“穿透”多个点来发现了，即便可以处理，查询速度也可能非常慢。我们用信用卡套现来举例。

首先是简单的直接套现模式。如左侧图显示，一个人办了一张信用卡，他其实不是真的想去还款，他找了一个商店，这个商店提供一个非法的服务就是信用卡套现。那么他通过信用卡付款，把 2020 元钱转到这个商店。这个商店直接就把其中的 2000 元钱返回给付款人，就完成了一次套现。这样的一种套现是非常简单的，我们通过对这个个体，对这个商店的收款记录和付款记录做分析，就可以识别出套现行为。

但右边这张图就复杂了很多。我们可以看到，右上角的这个人，他还是通过信用卡付款，付了 2020 元钱给了商店。这个时候，商店没有直接把钱退给付款的人，而是由一个个人付了 2000 元钱给到一个第三人。这个个人和商店之间，我们可以通过一些分析发现，他实际上拥有这个商店，所以我们把这种关系叫做同人关系。就是店和人虽然看起来是不同的实体，但其实他们之间有一个非常强的关联。那么他付款给的第三人也不是最开始刷卡的人，而是刷卡人的一个亲友，店主付款到了刷卡人亲友的银行卡上。那这样的一个套现模式就比左边的复杂很多了。我们把这种模式叫做多跳闭环模式。

要分析这种多跳闭环模式，就需要找出复杂的关联关系，而不能只对这个个体进行分析。但是大家可能会说，你画的这张图很简单呀，我一眼就能看出来，这是一个闭环，这个坏人我很快就能抓住。
但实际生活中情况可能会更加复杂，有更多其它交易和关系，就没那么容易看出来了。如下图所示，右边这张图可能会有千亿条甚至万亿条边，怎么很快地在这个图上把环找出来，这就对整个分析技术——复杂的关联分析技术提出了非常高的要求，性能成为了关键。

什么时候要用图计算

随着数据量和深度的增加，如果我们用传统的关系数据库的方法去分析的话，那就可能非常非常慢，难以在有效的时间内计算出结果。而图计算技术直接将事物与其关系像制作地图一样定位存储下来，直接支撑对事物和关系的各种查询和计算——这与我们大脑对信息的处理模式很像，大脑本身也可以建模成一个图。由于提供了对关联数据最直接的表达，以及图模型对异构数据天然的包容力，可以很好的解决目前遇到的关联数据分析问题。

由此可见，关系型数据库的设计擅长回答“已知”的问题，而图数据库可以回答超出设想的“未知”问题。相较于关系型数据库，图数据库是真正注重“关系”的数据库。

我们刚才举的是金融方面的例子，但是图计算的用途远远不限于金融行业。在互联网、工业领域、医药、公共卫生、公共安全等领域都有很多的应用。如绘制用户社交关系图谱进行社交影响力排名、好友推荐；构建设备关系网络图谱实现物联网建模分析、供电网络建模分析等。

随着互联网和 5G 时代数据指数级增长，数据之间的关系越来越复杂，企业管理和分析数据面临更高难度。越来越多的企业管理者们开始关注以图为代表的技术来更智能地使用数据，Google、Facebook 等科技巨头也早就在通过图数据库的力量来支撑主要业务应用。

国际知名咨询公司 Gartner，每年都会发布各种技术趋势的报告。在 2021 年的《十大数据分析技术趋势》报告中，Gartner 提到了“Graph relates everything”。这是一个非常有趣的双关，即图连接万物，一方面表示了图的本质，就是把各种事物连起来，另一方面也表达了图会在数据分析的各个领域得到广泛应用。Gartner 预测到 2025 年，图技术在数据和分析创新中的占比将从 2021 年的 10%上升到 80%。

蚂蚁集团开源图数据库 TuGraph，成立图计算开源委员会

9 月 1 日，2022 世界人工智能大会“新一代图智能技术发展与实践论坛”上，蚂蚁集团图计算负责人陈文光宣布开源蚂蚁集团高性能图数据库 TuGraph 单机版，并成立图计算开源技术委员会，中国工程院院士郑纬民、陈纯分别担任主席、副主席，5 位业界知名专家担任委员。

TuGraph 由蚂蚁集团和清华大学共同研发，是图数据库权威测试世界纪录保持者，也是世界上有测试纪录的“最快”的图数据库。

随着 TuGraph 的开源，图数据领域将迎来一款性能卓越、功能丰富、生态完备的开源产品。

开发者可以聚焦应用层，轻松打造属于自己的图数据，从而提升行业整体技术应用水位。TuGraph 开源采用 Apache2.0 协议，在 Github 和 Gitee 上进行托管。

图数据库区别于关系型数据库，基于图模型，使用点边来表示、存储、处理数据，拥有灵活的数据抽象模型，能够更好地表达出“关系”的概念。

蚂蚁 TuGraph 是一套分布式图数据库系统，可以支持万亿级边上的实时查询。此次开源的 TuGraph 单机版，同样具备完备的图数据库基础功能和成熟的产品设计，可以轻松支持 TB 级别数据和百亿级别大图，足以满足大多数业务场景需求。相较于市场上常见的开源产品，TuGraph 单机版的性能高 10 倍以上。

蚂蚁集团 2015 年开始自主研发分布式图数据库、流式图计算等图相关技术，2016 年发布自研分布式图数据库，并应用于支付宝。至今 TuGraph 已应用于蚂蚁内部 150 多个场景，包括在线支付的实时链路，以支付宝风险识别能力提升近 10 倍、风险审理分析效率提升 90%的成绩，验证了其高可靠性。

就在上个月，LDBC（关联数据基准委员会）发布最新图数据库 SNB 测试结果，TuGraph 在功能完整性、吞吐率、响应速度等层面全球领先。

目前，蚂蚁集团已形成了一套以图数据库为底座、同时包含流式图计算，离线图学习的大规模图计算系统。

蚂蚁集团图数据库负责人洪春涛表示，图技术是未来大数据、人工智能和高性能计算产业发展的关键所在，它很有可能会成为下一代的数据底座。蚂蚁集团愿意通过开源持续输出核心技术优势，推动图数据库更广泛的应用生态，携手行业抢占技术高地，不断探索技术的可能性。

技术解读 | TuGraph 图分析引擎技术剖析

图分析引擎又称图计算框架，主要用来进行复杂图分析，是一种能够全量数据集运行快速循环迭代的技术，适用场景包括社区发现、基因序列预测、重要性排名等，典型算法有 PageRank、WCC、BFS、LPA、SSSP。

TuGraph 图数据管理平台社区版已于 2022 年 9 月在 Github 开源，本文将对 TuGraph 图分析引擎的技术进行剖析。

1 TuGraph 图分析引擎概览

TuGraph 的图分析引擎，面向的场景主要是全图/全量数据分析类的任务。借助 TuGraph 的 C++ 图分析引擎 API ，用户可以对不同数据来源的图数据快速导出一个待处理的复杂子图，然后在该子图上运行诸如 BFS、PageRank、LPA、WCC 等迭代式图算法，最后根据运行结果做出相应的对策。 在 TuGraph 中，导出和计算过程均可以通过在内存中并行处理的方式进行加速，从而达到近乎实时的处理分析，和传统方法相比，即避免了数据导出落盘的开销，又能使用紧凑的图数据结构获得计算的理想性能。

根据数据来源及实现不同，可分为 Procedure、Embed 和 Standalone 三种运行模式。其中 Procedure 模式和 Embed 模式的数据源是图存储中加载图数据，分别适用于 Client/Server 部署，以及服务端直接调用，后者多用于调试。

Standalone 模式的数据源是 TXT、二进制、ODPS 文件等外部数据源，能够独立于图数据存储直接运行分析算法。

TuGraph 图计算系统社区版内置 6 个基础算法，商业版内置了共 34 种算法。涵盖了图结构、社区发现、路径查询、重要性分析、模式挖掘和关联性分析的六大类常用方法，可以满足多种业务场景需要，因此用户几乎不需要自己实现具体的图计算过程。


2 功能介绍

2.1 图分析框架 

图分析框架作为图分析引擎的“骨架”，可以联合多种模块有效的耦合协同工作。一般分为预处理、算法过程、结果分析三个阶段。

预处理部分用于读入数据及参数进行图构建及相关信息的存储统计，并整理出算法过程所需的参数及数据。

算法过程会根据得到的数据通过特定的算法进行逻辑计算，并得到结果数据。 结果分析部分根据得到的结果数据进行个性化处理（如取最值等），并将重要的信息写回和打印输出操作。

2.2 点边筛选器 

点边筛选器作用于图分析引擎中的 Procedure 和 Embed 模式。对于图存储数据源可根据用户需要和实际业务场景对图数据进行筛查，选择有效的点边进行图结构的构建。 

2.3 一致性快照 

TuGraph 中的 Procedure 和 Embed 模式能够提供数据“快照”，即建立一个对指定数据集的完全可用拷贝，该拷贝包括相应数据在某个时间点（拷贝开始的时间点）的镜像。由于 OLAP 的操作仅涉及读操作而不涉及写操作，OlapOnDB 会以一种更紧凑的方式对数据进行排布，在节省空间的同时，提高数据访问的局部性。 

2.4 块状读写模块 

块状读写模块作用于图分析引擎中的 Standalone 模式，用于对不同外部数据源的数据进行高效读入，同时也包含对内部算法处理后的图数据结果写回。 

2.5 参数模块 

参数模块作用于分析引擎中的 Standalone 模式，用于对图的一般信息（如数据来源，算法名称，数据输入、输出路径，顶点个数等）以及根据不同数据来源、不同算法所配置的不同信息参数进行接受和整理，传输给图算法及各个模块，同时将最终结果模块化展示。


3 使用示例

由前文所述可知，图分析引擎分为 Standalone、Embed 和 Procedure 模式，现在以 bfs 算法为例分别介绍他们的使用方式。

3.1 Procedure 模式 

Procedure 模式主要用于 Client/Sever 的 TuGraph 运行时，图算法的加载和调用。 在 TuGraph/plugins 目录下执行 bash make_so.sh bfs 即可在 TuGraph/plugins 目录下的到 bfs.so 文件，将该文件以插件形式上传至 TuGraph-web，输入参数后即可执行。 

示例： 在 TuGraph/plugins 编译.so 算法文件
bash make_so.sh bfs
将 bfs.so 文件以插件形式加载至 TuGraph-web 后，输入如下 json 参数：

即可得到返回结果。

输出内容解释： num_edges: 表示该图数据的边数量 num_vertices: 表示该图数据顶点的数量 prepare_cost: 表示预处理阶段所需要的时间。预处理阶段的工作：加载参数、图数据加载、索引初始化等。 core_cost: 表示算法运行所需要的时间。 found_vertices: 表示查找到顶点的个数。 output_cost: 表示算法结果写回 db 所需要的时间。 total_cost: 表示执行该算法整体运行时间。 

3.2 Embed 模式 

该种方式主要用于 TuGraph 在后台程序中对预加载的图存储数据进行算法分析，多用于快速调试。在 TuGraph/plugins 目录下对 embed_main.cpp 文件完善，补充数据名称、输入参数、数据路径等信息，示例如下：
保存后在 TuGraph/plugins 目录下执行 bash make_so.sh bfs 即可在 TuGraph/plugins/cpp 目录下的到 bfs_procedure 文件，bash make_embed.sh bfs

在 TuGraph/plugins 文件夹下执行./cpp/bfs_procedure 即可得到返回结果。

3.3 Standalone 模式 

Standalone 模式可以独立于图存储运行，直接从文本文件或 ODPS 读取 Edgelist 形式的图数据。在 TuGraph/build 目录下执行 make bfs_standalone 即可得到 bfs_standalone 文件,该文件生成与 TuGraph/build/output/algo 文件夹下。运行：在 TuGraph/build 目录下执行./output/algo/bfs_standalone -–type [type] –-input_dir [input_dir] -–vertices [vertices] --root [root] –-output_dir [output_dir]
	•	[type]：表示输入图文件的类型来源，包含 text 文本文件、BINARY_FILE 二进制文件和 ODPS 源。
	•	[input_dir]：表示输入图文件的文件夹路径，文件夹下可包含一个或多个输入文件。TuGraph 在读取输入文件时会读取[input_dir]下的所有文件，要求[input_dir]下只能包含输入文件，不能包含其它文件。参数不可省略。
	•	[vertices]：表示图的顶点个数，为 0 时表示用户希望系统自动识别顶点数量；为非零值时表示用户希望自定义顶点个数，要求用户自定义顶点个数需大于最大的顶点 ID。参数可省略，默认值为 0。
	•	[root]：表示进行 bfs 的起始顶点 id。参数不可省略。
	•	[output_dir]：表示输出数据保存的文件夹路径，将输出内容保存至该文件中，参数不可省略。

示例：在 TuGraph/build 编译 standalone 算法程序

在 TuGraph/build/output 目录下运行 text 源文件

得到运行结果：

结果参数解释同上。

4 小结

综上，图分析引擎可以高效、快速的处理多种来源的数据，其并行的图构建方式保证了内存占用小的特点。此外，图分析引擎也具有易于安装部署、灵活性高、耦合程度低、易于上手等对用户友好特性，可以帮助用户结合具体业务解决问题。

TuGraph开源JAVA客户端工具TuGraph-OGM，无缝对接JAVA开发生态

TuGraph 图数据库提供了 JAVA、C++、Python 等多种语言的 SDK 支持，方便客户在各种场景下使用。用户使用 SDK 向TuGraph服务器发送Cypher请求，服务器则以 JSON形式返回数据。近日，TuGraph 推出了一款面向 JAVA 客户端用户的开发工具 TuGraph-OGM (Object Graph Mapping)，为用户提供了对象操作接口，相较 Cypher/JSON 接口应用起来更加便捷。

OGM 类似于关系数据库中的 ORM（Object Relational Model），可以将数据库返回的数据自动映射成 JAVA 中的对象，方便用户读取，而用户对这些对象的更新操作也可以被自动翻译成 Cypher 语句发送给服务器。这样即便是完全不懂 Cypher 的用户，也可以通过操作对象与数据库进行交互，大大降低了图数据库的使用门槛。

TuGraph-OGM 同时也兼容其他开源产品 OGM 工具如 Neo4j-OGM，方便用户将工程在不同数据库与 TuGraph数据库间无缝迁移。本文将对 TuGraph-OGM 进行全面的介绍。

0 映射原理

TuGraph-OGM 将 JAVA 对象映射为图的对象，类映射为点，类的属性映射为图中的属性，类中的方法映射为操作 TuGraph 的查询语句。

以电影场景为例，对演员、电影、导演之间的关系进行数据化，就形成了非常典型的图数据。举一个简单的示例，演员Alice在1990年和2019年分别出演了两部电影《Jokes》和《Speed》，其中《Jokes》的导演是Frank Darabont。
以图的思维来看，演员、导演、电影可以被映射为三种不同的节点，而出演、执导可以被映射为两种边，映射结果如上图所示，将数据存入图数据库后，相关的开发人员就可以使用各类图查询语言对数据进行查询。
但对非图数据库相关的开发人员来说，这个例子中的演员、导演、电影作为实体，同样可以映射为类中的对象，而与实体相关联的对象可以通过集合存储，这是大多数开发人员熟悉的领域。

1 TuGraph-OGM架构

TuGraph-OGM可被看做一个"翻译器"，主要功能是将开发人员对JAVA对象的一些操作翻译为TuGraph可理解的图查询语言Cypher请求，并将该操作返回的结果，再次翻译为JAVA对象。架构图如下所示：

3 使用示例

详细示例请参考tugraph-ogm-integration-tests 在pom.xml中引入依赖

3.1 构建图对象

首先需要通过注解标明图中的实体。

@NodeEntity：该注解标明的类为节点类。

@Relationship：用于标明边，同时@Relationship中可指定label与边的指向。

@Id：用于标明identity，是OGM中数据的唯一标识。

3.2 与TuGraph建立连接

当前TuGraph-OGM提供了RPC driver用于连接TuGraph，具体配置如下所示：

3.3 通过OGM进行增删改查

OGM支持对TuGraph的实体执行CRUD 操作，同时支持发送任意TuGraph支持的Cypher语句，包括通过CALL调用存储过程。

CREATE

在完成图对象的构建后，即可通过类的实例化创建节点，当两个节点互相存储在对方的集合（该集合在构建时被标注为边）中，就形成了一条边，最后使用session.save方法将数据存入数据库。
注意：TuGraph数据库为强schema类型数据库，在创建实体前需要该数据的label已经存在，且新建过程中需要提供唯一的主键。

DELETE

使用session.delete方法删除节点，同时会删除与节点相关联的所有边。

UPDATE

修改已创建的节点的属性，再次调用session.save方法会对节点进行更新。

MATCH

session.load方法用于根据节点id查找节点。 session.loadALL方法用于批量查找节点，支持通过多个节点id查找节点、查找某一类型的所有节点、带有filter的查询。 filter查询需要新建Filter，传入参数ComparisonOperatorx0;可选为：EQUALSx0;、GREATER_THANx0;、LESS_THAN

QUERY WITH CYPHER

OGM支持通过queryForObject、query方法向TuGraph发送Cypher查询，由于Cypher查询的灵活性，需要用户自行指定返回结果格式。
session.queryForObject方法：需要在方法第一个参数处指定返回类型，可设定为某一实体类或数字类型。
session.query方法：Cypher查询的返回结果被存储为Result类型，其内部数据需要用户自行解析，以下方代码为例，传入数据库的Cypher为CREATE查询，返回结果createResult可被解析为QueryStatistics，可获取到此次查询被创建的节点与边的数目。

关于TuGraph

高性能图数据库 TuGraph（https://github.com/TuGraph-family/tugraph-db） 由蚂蚁集团和清华大学共同研发，经国际图数据库基准性能权威测试，是 LDBC-SNB 世界纪录保持者，在功能完整性、吞吐率、响应时间等技术指标均达到全球领先水平，为用户管理和分析复杂关联数据提供了高效易用可靠的平台。

历经蚂蚁万亿级业务的实际场景锤炼，TuGraph 已应用于蚂蚁内部150多个场景，助力支付宝2021年资产损失率小于亿分之0.98。关联数据爆炸性增长对图计算高效处理提出迫切需求，TuGraph 已被成熟应用于金融风控、设备管理等内外部应用，适用于金融、工业、互联网、社交、电信、政务等领域的关系数据管理和分析挖掘。

2022年9月，TuGraph 单机版开源，提供了完备的图数据库基础功能和成熟的产品设计，拥有完整的事务支持和丰富的系统特性，单机可部署，使用成本低，支持TB级别的数据规模。

蚂蚁高性能图数据库 TuGraph-DB 的技术思考与实践

在近日举行的  DTCC 2022  第十三届中国数据库技术大会-图数据技术与应用创新专场，蚂蚁集团图数据库负责人洪春涛博士分享了蚂蚁高性能图数据库 TuGraph-DB 的技术思考和实践，以下为演讲内容要点回顾。
图计算的优势

图计算是一种高效的抽象计算方法，可以方便地处理复杂的多维数据。我们将员工和公司之间的关系画成图，这样可以快速查询员工的信息，例如员工的工作情况、与其他员工的关系以及与哪些公司有联系。相比之下，在关系数据库中，我们需要分别建立员工信息表、公司信息表以及员工和公司之间的关系表。这就把整个数据切成了很多张二维表，在实际应用系统中经常能够看到几百上千张表。这样会使得系统变得更加复杂，需要基于多张表去做推断，对人或机器来说都会带来挑战。

对比来看什么是简单查询和复杂查询：

图中表格的前两行属于比较简单的查询，比如某个员工的工龄，直接找到对应的那一行然后取出来就可以；比如找出所有的雇员数大于 5 的公司，可能在雇佣关系表中就能找出来，在关系数据库里这些都属于常规的操作。

但如果查询再复杂一点，我想知道员工 A 和员工 C 之间有什么关系，不一定是一跳的关系，可能包含在同一家公司工作，也可能包含参与了同一个项目，甚至还包含他们有一个共同好友，这些关系就是多种多样的，想用 SQL 列举所有可能的关系其实就没有那么容易了。如果确定知道这里可能有几种关系，还能靠 SQL 穷举出来，但随着表越来越多，里面的关系可能有几百上千的时候，再想去穷举就非常难了。

最后还有一种更复杂的查询，比如想找员工 A 和员工 E 的所有关系，可能包括 A 认识 B，B 认识 C，C 认识 E，相当于在做一个不定长跳数的查询，在 SQL 里面就非常难写了，相信绝大多数人都难以写出这种查询。

业界有句话，所谓的关系数据库实际上并不擅长处理关系。例如，员工 A 和 E 之间的所有关系实际上就是我们要查询的关系，但在关系数据库中处理这种关系查询实际上是不够友好的。这是图数据库的优势所在，它们更擅长处理复杂的关系数据。我们发现，大部分浅显的数据信息都可以比较容易地挖掘出来，但要想更深入地利用数据产生更多的价值，就必须挖掘更深层次的信息，这时就一定会遇到这些复杂的关系，这也是为什么图数据库越来越受欢迎的原因。


为什么图数据库开始流行

图数据库的概念最早出现在七八十年代初期，但当时为什么人们没有选择使用图数据库，而是选择了关系数据库呢？我认为主要原因是，当时的计算机并不那么强大，使用二维表格形式的关系数据库对计算机来说更友好。我们知道，计算机最擅长的事情就是重复的劳动，重复的任务。如果我们要在一张二维表中找一行数据，我们可以一行一行查找，或者使用二分查找（如果数据是树状有序的）。每一层都是重复的算法在运行，这对计算机来说是一个非常规整的数据，容易处理。但如果把数据抽象成一张图，那么难度就会大很多，对计算机来说会更难处理。

举个例子，一个员工可能与其他人有各种不同的关系，比如朋友关系、雇佣关系、参与项目关系，每种关系的类型都不同，对应的数据也都不同。此外，一个点上的边数也非常不规律，有的人可能只有几个好友，而在微博上，一个普通人可能有两三百个粉丝，而大 V 账号可能有数百万甚至上亿的粉丝。这样一来，存储数据时，普通人和大 V 之间的差距就非常大了，对计算机来说，处理这两种数据的差异也会很大。

我们知道，在七八十年代，计算机相对来说很弱，如果那个时候使用图来表示数据，整个处理和查询的难度就会大很多。所以，人们选择使用关系数据表的形式表示数据。事实上，那个时候有人做过图数据库，但并没有流行起来，原因就是性能相对关系数据库来说差得太多了。

我们知道，所有的事物，尤其是电脑，一直在不断进步。这包括硬件和软件方面的改进。如今的硬件和几十年前的硬件完全不是一个概念，而现在的软件优化也大不相同。随着这些改进，我们会发现对当前的电脑和计算机系统来说，使用图数据库带来的额外开销可能不是很大的问题。

举个例子，我们会发现，过去人们都使用低级编程语言编写程序，但随着时间的推移，有了高级语言。比如最开始、最原始的电脑可能是用纸袋和机器码写程序，后来有了汇编，再后来有 C 语言、C++，现在很多人都直接写 Python。虽然  Python  程序的执行速度可能较慢，但是写的很快，而用用 C++或者汇编去写得写半天，对于编写程序到最终得出结果的整个过程来说，使用  Python  会更方便。

计算机编程语言的发展是从低级向高级演变的，数据抽象也是一样。我们认为，未来的数据抽象一定会对人更友好一些，而不是专注于对机器更友好。如图所示，编程语言的发展是从低级语言向高级语言转变的，我们也认为数据抽象层次也会慢慢从低层次表格抽象向高层次图表抽象发展。

图计算在蚂蚁的应用

自 2015 年开始，蚂蚁实际上投入了大量资源来研究图计算，研究如何在蚂蚁的业务中使用图计算。例如数据血缘应用。在对业务的处理过程中，我们需要较好地追踪这些数据的流转路径，如果修改了一份源数据会对下游数据产生什么影响，会对最终业务产生什么影响。为了更好地追踪，我们使用图数据库来存储数据。

另一个比较有趣的应用场景就是程序分析。相信几乎所有互联网公司内部都有大量的程序，因此，我们需要管理这些程序，并在每次提交代码时了解将会对哪些内容产生影响。为此，蚂蚁负责程序分析的团队会分析这里的图数据。例如，定义一个变量 A，然后使用变量 A，“定义”与“使用”之间就会有一条边，使用关系会存储在图数据库中。目前我们的图中已经有超过 200 亿条边，这是一个非常大的数据量。我们需要对这些数据进行存储、查询和分析，这是蚂蚁公司内部非常多的图数据场景之一。
举个例子来说明优惠券反套现的场景：满额返券是一种比较常见的促销方式，比如购物满 2000 元就可以享受 100 元的优惠。这种情况下，如果正常消费，用户花费 2000 元，通过返券省下 100 元。但是有些人会想办法注册假商铺，进行虚假交易，目的是把平台补贴的优惠券套出来。因此当用户去买东西，平台要去付补贴的过程，我们需要去实时检测一下会不会有可疑的资金交易情况。

蚂蚁有很多业务需要研究图计算系统和图数据库等技术来满足需求，因为这些业务需要对大量的点边进行分析，数据量超过了 100TB，基本上已经达到了 PB 级别。我们需要对这些图进行实时查询，吞吐率大约在百万级别。由于需要对用户的付款进行实时判断，所以需要比较低的延迟，大约在 20 毫秒的级别。如果延迟太长，会导致用户体验很差，比如付款需要等待 5 秒才能完成，这样就会非常麻烦。

图计算系统建设中的问题与挑战

在建立蚂蚁图计算系统的过程中，我们遇到了各种各样的问题。为了解决这些问题，我们与学术界和许多研究界的同事一起合作，并发表了许多相关的学术论文，包括 EuroSys 等。然而，我们在建立系统的过程中发现，目前的图计算仍处于较早期的阶段，因此许多标准尚未成形。这对我们来说是一个棘手的问题。例如，在关系型数据库中，查询语言基本上就是 SQL，但在图数据库中，仅查询语言就有许多种，包括 Gremlin、G-SQL 等等。这导致了市场的碎片化，人们学习和使用的成本也很高。

在建立图计算系统的过程中，我们也遇到了许多挑战。为了分担较大的通信量，需要将图数据分布到多台机器上，但这会导致边的信息在不同机器之间传递，造成大量的通信。此外，单次查询所涉及的数据量也比较大，例如五跳查询涉及的点数就已达到 10 的五次方，图中还存在一些非常大的点。同时，用户对图计算系统的需求也十分多样，既有快速查询的需求，也有对复杂算法（如社区发现）的需求，单一系统很难满足这些不同的需求。

TuGraph 技术优势

蚂蚁自己开发了一套图计算系统 TuGraph，既能解决图数据的存储问题，也能解决流式计算、离线计算和图学习的问题。目前，超过 100 个业务线和 300 多个场景都在使用这套系统。这套系统在 2021 年获得了世界互联网大会领先科技成果奖。

在 TuGraph 中，性能是一个重要的因素，因为图数据集的体积很大，如果性能不佳就会浪费机器资源，导致许多情况下无法完成任务。比如，希望业务的查询能在几十毫秒内返回结果，但是如果做的性能不好，几秒钟才能返回结果，就无法作为在线查询使用。因此，我们是非常对性能是很重视的，其中在 LDBC-SNB 标准测试中（类似于数据库领域性能标准测试 TPC-C），TuGraph 仍然是世界纪录的保持者。

TuGraph 的整个图存储是建立在完美哈希的基础上的，这是我们与其他图系统的一个重要区别。目前，大多数图系统使用的是基于数的存储，但数的问题在于永远存在一个 LogN 的查找操作。然而，在图中可以看到，不同的顶点之间实际上是无序的，不需要有顺序，所以顶点这个级别实际上是基于哈希的，理论上，顶点的读取是最优的。

此外，TuGraph 还参与了许多标准的定制，整个系统在尽量往标准化的方向去做。

除了为内部提供服务，我们还向外提供服务，主要是因为，作为一个系统，如果只为有限的客户提供服务，就很容易构建成一个专有系统。我们希望这是一个标准化、开放的系统，所以我们也在对外提供图计算系统的产品和服务。目前，我们也有很多外部客户，包括金融、工业、互联网以及政企领域。

开源开放，共建发展

整个图计算系统目前仍处于较早期的阶段，我们认为还有很多工作要做，包括提升应用性、性能和降低成本。所有的系统都会有这些问题。但是，如果希望普及，我们认为最重要的是有健康的生态，来推动图计算系统的发展，需要有更多的用户和更多的场景使用这个系统。
所有的计算机系统都需要去有一个更开放、更大的生态才能促进发展。蚂蚁有一句话叫做“成熟一个、开放一个”，一个系统成熟以后，我们就会试着开放出去，让更多的人去用。今年 9 月，我们已经在 GitHub 上开源了 TuGraph 中的单机版图数据库，以及一个离线图分析引擎 TuGraph Compute。分布式图数据库和流式图计算现在已经包含在我们的商业化版本中，包括一站式图研发平台。我们计划在未来迭代更多更丰富的系统功能，希望能做得更好。

TuGraph 开源版特色

为什么要去开源单机版而不是分布式版本？主要是考虑到它的部署和使用成本比分布式版本要低得多，同时功能也很完整、独立。我们希望这样可以让许多刚开始使用图数据库或有使用图数据库解决问题的想法的人，可以先尝试用我们的单机版图数据库。因为它的部署非常简单，如果跑起来没有问题，那么再考虑是否需要分布式版本。如果确实需要，我们可以再跟进这个问题。

我们的单机版图数据库已经能够支持 TB 级别的数据，我们内部也有很多情况使用单机版图数据库。在单台机器上，我们最大的数据量也达到了 2TB 多，在线上运行，能够处理百亿级别的点边。事实上，大多数用户使用单机版图数据库都是足够的。由于单机版的图数据库很容易优化，我们对它进行了极致的优化，因此单机版图数据库在性能上可以满足绝大多数场景的需求。此外，它的系统特性也很全面，包括高可用性、多图支持、权限管理、日志记录等，它可以被看作是一个成熟、易用的图数据库，类似于 MySQL。

图中所列出的开源版 TuGraph 几个特性包括：

	•	单机版图数据库能够处理数据量几个 TB 的数据，前提是磁盘足够大。
	•	成本很低，因为是单机版，部署和运维都很容易。
	•	性能很好，我们对其进行了大量优化。TuGraph 的 LDBC-SNB 测试目前是世界第一，大家可以在 GitHub 上获取测试 SNB 的条款并进行测试。
	•	单机版图数据库是一个非常易用的完整系统，我们提供了导入导出工具和查询语言。此外，还提供了底层 API，用户可以使用它来编写复杂的程序。
 
我们的开源版本的目标主要有三点：

首先，我们希望提供一个免费的图数据库产品，能够让更多的人使用图数据库，尝试用它来解决问题。

其次，我们希望促进图数据库标准的成形。目前图数据库的差异太大，每个数据库都有所不同，我们希望通过提供一个参考答案来帮助大家达成趋同。这样大家就可以根据我们提供的设计来判断哪些特征合理，如果觉得合理就可以遵循这个设计，慢慢地大家就会逐渐靠近。假如所有产品在主要特征上保持一致，这样所有人的学习成本就会降低。

最后，基础研究性问题可以不断优化发展，包括存储方面的问题，例如哈希可能是理论上最优的，但是是否还有其他需要调整的东西？目前没有一个很好的研究性平台让大家去进行这些尝试和研究，我们希望提供的开源 TuGraph-DB 能成为这些研究人员的对比基线，促进研究的发展。

TuGraph 企业版特色

除了开源版本，我们也继续提供商业版本。这个版本包含一个分布式图数据库，以及离线计算引擎和流式图计算功能。此外，我们还提供了 TuGraph Platform 一站式图平台，包括运维、可视化等功能。在这个平台上，用户可以在图数据库中执行流式计算，并在线写回数据库。这种方式通常用于实时查询结果，因为流式计算的时间可能比较长，但用户可以立即查询到较早的结果。这对于在线业务来说非常重要。

商业化产品还提供私有化部署，也可以通过一体机的方式部署硬件，并将很快推出云上部署方案，这样大家就可以在云上体验我们的产品。

总结

蚂蚁在图计算方面投入了大量资源，并在众多业务场景中磨练出了一整套在线查询、流式计算、离线分析以及图学习的体系。目前，我们已经在 GitHub 上开源了单机版（https://github.com/TuGraph-family），同时也提供企业版来满足不同用户需求。

陈文光：AI时代需要怎样的数据处理技术？

大家好，我是清华大学/蚂蚁技术研究院陈文光，今天为大家带来《AI 时代的数据处理技术》主题分享。

我们身处一个以信息技术为核心驱动力的大数据时代。从下面这张图，我们可以看出，数据量和数据生成的速度在飞速增长。与此同时，新的产生数据的形式在产生，数据模态也在不断增长，不仅包括自然语言，还有声音、图像、视频等等多种形式。最近非常流行的多模态大模型包括具身智能、触觉等新的数据形态。

丰富的数据形态要求我们要对数据做有效处理。模仿“马斯洛需求层次理论”，数据处理也有一个层次，从最底下的收集数据、存储数据到做一般的数据查询处理，更上面的层次是现在越来越接近于用 AI 方式处理数据，甚至最后还能生成很多内容。

1.AI时代数据处理新需求

这样的大模型崛起的时代也引发了对数据处理的新需求。最近，Meta 出了一个新模型 LLaMA-3，效果非常好，它实现了在十几万亿的 Token 上面做的训练，而我们之前很多模型可能只是在 4T 的或者几个 T 的 Token上面做训练。

那么，如何获得增加的这部分 Token？实际上，这需要从很多网上低质量的数据中做大量的数据处理，清洗出来可用的高质量数据，如果想让大模型的能力进一步增长，实际上需要数据处理做很多的工作。

另一方面，大模型直接应用在生产服务场景下，本身还存在很多缺陷，比如幻觉问题、上下文长度的问题。目前的多数超长上下文大模型并不能完整记录真正领域的知识。为了满足需求，向量数据库和大语言模型结合起来，提供高质量的服务。 
从数据服务的角度来讲，向量数据库是一种使用嵌入的方式表达知识，再用另外索引的方式快速找到相应知识的方式，它和大模型配合才能获得很好的效果。所以大模型的发展和崛起，对数据库领域也提出了很多新需求。

2.ai时代数据库发展趋势

在这样的趋势下，我今天想分享三个观点，也是未来的数据库面临的三个比较重要的发展趋势：

（一）在线离线一体化

这张图是企业常见的在线、离线两个链路。

	◦	上面是在线链路，一个数据请求会先经过预处理，再通过训练好的模型做推理，比如风控、分类等等，再把结果反馈到 KV 里，直接服务用户的请求。
	◦	下面是离线链路，收到数据请求后，我们要想办法处理，去更新模型。经过一段时间后再把模型更新到在线链路上。

在线、离线两个链路分开，在生产中会遇到一些比较严重的问题，主要就是在线、离线不一致。

比如在离线链路上做了各种仿真模拟，但是当把策略、模型上传到在线链路时，会出现与离线链路仿真效果不一样的情况。造成这种现象的根本原因，就是我们通过不同的数据链路把新数据接入进来，离线链路处理的数据与在线链路不一致。

怎么解决这个问题？最好的解决方案，就是只有一套数据。如果能够做到一个系统既能够做事务处理，可以支持事务在数据上面原子化地做更新，同时还可以在这一批数据上做后续分析型的业务。也就是说用一份数据给在线离线链路的一致性打下基础。
这里面也有非常多的技术问题。一般来说，事务处理行存会比较好，分析一般是列存比较好，当一套系统需要同时支持行存和列存的时候，需要什么样的存储结构？另外，事务处理对于优先级、延迟/尾延迟、吞吐率要求比较高。那么在系统里如何调度不同优先级的请求，这里涉及到很多相关技术。

在蚂蚁的图风控中，也有另外一个场景。刚才我们讲到，可以通过数据库本身的 HTAP 引擎解决在线/离线一致性的问题，如果没有这样的混合系统，应该如何实现两份数据达到在线离线一致性？下面以图风控方案中的在线离线一体化为例，给大家介绍。
TuGraph DB（分布式图数据库），是一个支持 事物处理 的图数据库。TuGraph Dataflow （流图计算系统），可以看作是一个支持图语义的 Flink。

在我们原来的方案中，这两个系统采用不同的查询语言，一个是我们自定义的 GQuery 语言，另外一个是基于 Java 的支持 Gremlin 语言。这两个系统的数据通过 TuGraph Dataflow 处理完成后，一条线通过 TuGraph DB 去做在线链路，另外一个经过存储去完成后续的离线分析，这时就会出现数据不一致的情况。

这个问题如何解决呢？首先需要让数据保持一致性。数据虽然是两份，但是在TuGraph DB 和存储之间新增了一条数据同步链路，就是通过从 Binlog 中读取数据，保证两份数据的一致性，防止出现两边写，一边写成功、一边写失败，而导致的数据不一致。当把在线数据里已经处理完成的数据同步至离线数据，这时数据的一致性是有保证的。
另外，我们把这两个系统的查询语言和语义都统一起来，都使用国际标准图查询语言 ISO-GQL，同样一套查询语言在两个系统上用同一个语义支持，在进行后续的策略分析时，数据和查询语言的语义是一致的，可以达到更好的一致性。

这里也存在非常复杂的情况。图数据库的基本功能是从一个点扩出去很多点，但是有些点的邻居非常多，可能有几十万、上百万个，所以我们会限制每个点扩展的点数，比如只扩两百个。但同时还需要在两个系统中保证不仅只扩两百个点，这两百个点都是一样的，才能保证数据一致性。所以想要在两个系统中要保证数据一致性，需要花费相当大的精力。

（二）向量数据库和关系型数据库一体化

向量数据库和大语言模型的结合有非常重要的作用，如果一个企业要用大语言模型做服务，既要部署语言模型又要部署向量数据库，同时企业的很多数据又保存在关系型数据库中。

这样一个多系统复杂混合的部署，开发、部署、维护非常困难。因为涉及到多个系统之间的依赖性，软件版本、系统之间的交互也会存在很多的问题。如果能够把这些功能做到一起，就能够实现一致性的管理。
在关系型数据库中，可以通过一些插件支持向量数据库的语义，同时在调用查询引擎的时候，将数据分到不同的链路上执行，从用户的角度就可以实现只部署一个系统，使用一套语言，完成相关工作。

蚂蚁集团有一套内部的 VSAG 的向量库，实现了主流向量数据库的相关功能，而且在实际生产中已经得到应用。向量数据库最有名的是 FaceBook 开源的 FAISS 系统。

VSAG 和 FAISS 之间有什么区别？FAISS 功能非常强大、性能非常好，对 GPU 也做了很多优化，但是相对来说提供了很多底层功能，这就需要通过调整各种参数、配置，从中得到一个对应用比较合适的配置。

而蚂蚁集团的 VSAG 库更多从开发者和产品应用性的角度出发，默认把很多基础配置的事情都做好了，而且在 CPU 上也实现了很多优化，提供了近似于开箱即用的功能。
在 OceanBase 里，以插件的方式集成了 VSAG 功能，可以在 OceanBase 里使用 VSAG 向量化的功能，用一套系统达到这样的效果。

（三）数据处理与 AI 计算一体化

有人可能会问，数据处理不就是 SQL 吗？AI 是神经网络层面的东西，AI 与 SQL 为什么会结合到一起？我举一个例子。大家知道世界上有很多的网页，网页上面有很多内容，内容量非常大，远超几十 T、几百 T。但是在这些海量内容中，很多内容质量很低，如何从中提取出高质量的内容？FaceBook 提出了一套 CCNet 的流程，下图的 CCNet 流程展示了数据处理和 AI 的模型在这一过程中的融合试用。

第一步， CCNet 对网页的原始数据进行解析，在 HTML 的网页中抓取内容，这里涉及到解析等工作。

第二步是删冗，删冗也可以被认为是一个 JOIN，因为抓取网页内容中可能用到了别的网页内容，语料里面有冗余不利于最后的训练，即对每段话都做一个哈希，和过去已有的内容对比，是相同还是不相同。解析与删冗是非常典型的数据处理过程。

第三步，做语言分类，需要经过神经网络模型判断网页的语言。

接下来，通过一些 AI 模型对数据做分词、质量评估，后面的过滤、分桶工作，又回到数据处理。

在这个应用里，数据处理和 AI 计算处于交叠的状态，不是一次数据处理之后都交给AI 完成后续的处理，这是一个复杂的来回交互的链路。

那这种情况下，什么样的系统可以支撑这样复杂交互的服务？现在的 AI 和大数据生态基本是割裂的生态：

	◦	AI 用 Python，主要用 GPU ；
	◦	大数据基本上是用 CPU ，用基于 Java 的 Spark 实现。

另一方面，在很多小数据的处理上，Python 已经展现出非常强大的性能，像 Pandas 这样的系统，在单机数据的处理上提供了非常方便的接口。

当 AI 逐渐成为主流计算形态的时候，数据应该如何与 AI 融合？

由于 Spark 是基于 Java 的生态，当我们如果把大模型处理交给 Spark 去做，它产出的结果要通过文件系统、或者其他传输方式交给 AI 的 Python 程序，Python 处理完之后可能还有一些后续处理。在刚才的例子里面，数据处理和 AI 计算之间会有多次的交互，对整个系统的开发、调试、部署、维护带来非常大的问题。

有人尝试把数据处理和 AI 结合起来。2019 年，英特尔出了一个系统“BigDL”，在 Spark 里面把神经网络的描述、优化器、训练方式把这些东西加进去。当时只支持了 CPU，而且是基于 Java 的。我们可以认为，这种方式是试图把 AI 融入到大数据的生态里面。

我们反过来看，如何把大数据的生态往 AI 的方向牵引？这其实是 Spark 的 Python 化。

上图是 2022 年在DataBricks Summit上讲的。这是一个分布式的 PySpark，就是 Python 接口的 Spark系统。当时 PySpark 的使用率已经达到了整个 Spark 使用率的近 50%，很多人已经愿意用 PySpark 了。但是 PySpark 还存在一个问题：它的性能很差。

Python 是一个动态语言，在编译时不知道它的类型，动态时才知道，所以它的性能很差，比 Java 的 Spark 还要慢一半。所以虽然 PySpark 对编程非常友好，很多人也习惯用，但是性能不太好。因此我们在处理大量数据的时候，希望能够避免这一问题。

所以，我们提出一个愿景，融合数据处理和 AI 生态。

我认为还是要基于 Python，因为 AI 是主要的计算形式，所以整个数据处理应该围绕 AI 建设。从编译优化的角度来讲，我们希望把 PySpark 做很多的优化。这件事是可以做的，我们最近也有了一些成果。在删冗部分，通过把 PySpark 做相关优化，基本上性能可以提升一倍多，可以达到我们的性能预期。

未来，这个生态不只是编程要融合，底层的硬件也要融合，数据和 AI 结合以后，底层的硬件生态也要支持 GPU、弹性任务调度，最后可以达到“一次编写到处执行”的效果。



3月30日，TuGraph 社区 Meetup “图数据库智能化建设与探索”在北京顺利举办，探讨大模型时代下的图数据库智能化。
 
01. 技术分享｜TuGraph计算引擎模型推理系统设计与优化

“虽然传统的基于迭代的算法能够解决我们现实生活中的大多数问题，但随着业务需求的不断发展和现实问题的日渐复杂化，这些算法往往难以满足某些具体的需求。尤其是当规模和维度日益增长、数据越发密集时，我们很难利用这种传统的方法去提取到更加关键的一些信息，或者说是我们从人的视角上更难理解的一些信息。因此，由于图结构在表达能力上的优势，结合机器学习分析技术，图算法近年来引起了广泛关注，并在业界落地和取得了较好的商业价值。”
 
02. 技术分享｜TuGraph-DB兼容Neo4j客户端：Bolt协议设计与实现

“兼容Neo4j客户端的最大优势在于生态支持。以客户端为例，Neo4j官方自身支持五种编程语言的客户端，社区又贡献了两种，共计七种语言的客户端得以直接使用。此外，还有一系列与上下游生态相接的组件，如与Apache Spark或Apache Kafka的连接，都有现成的代码可供利用。在编程框架方面，特别是Java，例如OGM（Object-Graph Mapping，对象图映射）以及一些业务开发框架，如Spring，这些所需的相关代码都已现成，无需重新编写。这种做法极大地节约了研发资源，我们可以将这些资源重新投入到提升数据库本身能力上。”
 
03. 技术分享｜知识图谱语义框架SPG及图谱推理

“当前，我们正处于图谱技术发展的第三阶段，这一阶段的核心是将图谱与大型模型相结合。目标转向了知识的标准化、跨领域数据的联通与复用。随着这个阶段的深入，简单地在推理过程中融入文本概念和信息，或者是加入交易与社交的实体关系，已经不能明显提升推理效果了。关键的做法应当是结合实体信息的多元素特征进行深度协作，从而更精准地关联相关性，揭示那些稀疏的实体间关系，并实现意义解释的密集化。”
 
04. 技术分享｜CStore Compaction模块的设计与优化

 “TuGraph Analytics本质上是一款图分析OLAP数据库。CStore作为一个单机版存储引擎，提供了坚实的存储基础。同时，RocksDB也可以作为TuGraph Analytics的存储基础。我们采用LDBC提供的通用社交网络图数据集进行了基准测试，测试涉及让TuGraph Analytics分别连结RocksDB以及我们自有版本的CStore进行分析。在同步与异步compaction（数据压缩整理）两种方式下进行了读写性能测试：同步方式意味着数据写入完成后进行compaction，完成之后再进行读性能测试；异步方式则是写入和compaction同时进行，写入完成后立即测试读性能。在这两种情境下，使用CStore的TuGraph Analytics的读性能超过了使用RocksDB的三倍以上。” 
 
05. 社区规划｜TuGraph 社区技术路线
 
最后是展望未来环节，TuGraph 开源负责人范志东与大家分享了大模型时代的图计算要做些什么，包括Q2即将推出的开源数据分析工具 OSGraph，Q3即将开源的 TuGraph 研发平台 TuGraphMaker，结合大模型的“与图对话”工具 ChatTuGraph 等项目。
06. 圆桌讨论｜图技术、图生态、图智能在自由讨论环节，TuGraph 布道师戚仕鹏邀请了几位 TuGraph 的老朋友一起聊聊图技术、图生态、图智能。包括中国开源先锋人物 、华为产业发展专家、Rust 技术专家马全一老师，北京大学前沿交叉学科研究院数据科学博士庞悦，蚂蚁集团知识图谱专家王少飞，以及TuGraph 开源负责人范志东。各位老师就为什么开源、图技术的未来与学术热点、图与AI等话题进行了精彩讨论。2024年，TuGraph 将努力更贴近客户，更拥抱开源，更关注生态。欢迎大家继续关注！


权威报告：蚂蚁集团TuGraph跻身中国图数据库市场“领导者”象限

7月15日，全球领先的IT市场研究和咨询公司IDC发布了最新的市场研究报告《IDC MarketScape：中国图数据库市场厂商评估，2023》。报告显示，蚂蚁集团自研的企业级图数据管理平台TuGraph跻身"领导者"象限。     

IDC在报告中列举了蚂蚁集团TuGraph的五大优势。第一，TuGraph研发七年多来支持了蚂蚁集团300多种应用，在大量不同场景中长时间使用，产品成熟度、丰富度等方面具有优势。第二，具备业界鲜有的在线、流式、离线"三线一致"计算能力，覆盖了毫秒级延迟、分钟级和小时级等不同引擎，满足不同场景的性能需求。第三，支持每秒千万级查询的超高吞吐，毫秒级超低延迟。第四，生态工具“一站式图研发及可视化平台 TuGraph-Platform”降低了用户使用门槛。第五，在其他非金融场景，如社交推荐、数据血缘管理、故障归因分析等场景广泛使用，应用体系成熟。

本次报告IDC主要评估了中国市场上12家典型的企业级图数据库厂商，类型覆盖互联网厂商、云服务厂商、大数据厂商等。IDC针对入选图数据库厂商的产品能力（Capabilities）和技术战略（Strategies）两个维度，考察了目前的产品技术能力、市场和生态以及未来发展战略等，评估了16个细分能力指标和4个细分战略指标，包含72个指标评估项，并配以不同权重。该报告为企业发展图计算和投资选型提供了有力支持。

图数据库，即以图（Graph）数据结构来进行存储和分析的数据库。与传统数据库相比，图数据库擅长关系分析，能更好地管理和组织数据，开发上层智能模型，同时也能实现海量数据的高并发、低延迟分析处理，提高数据变现的商业价值。当前图数据库应用主要集中在欺诈检测、人际关系分析和预测分析等领域。
 
IDC调研发现，数字经济、产业数字化转型进入深化发展阶段，企业对于业务逻辑开发和关系挖掘需求增强，图数据库市场受大型央国企数字化转型政策的推动明显。95%的企业认为图数据库是重要的数据管理工具，超过 65%的厂商认为在业务上图数据库优于其他选择。整体来看，图数据库的使用仍处于早期阶段，仍然缺乏统一标准范式，技术生态环境弱势，低操作门槛的图计算平台仍有较大缺口。

蚂蚁集团从2015年开始布局图技术，与清华大学合作研发了高性能图数据库TuGraph，扛住了蚂蚁万亿级业务的高性能要求。TuGraph在功能完整性、吞吐率、响应时间等技术指标上处于全球领先水平，两次打破国际图数据库基准性能测试世界纪录，成为LDBC-SNB世界纪录保持者。TuGraph在2021年帮助支付宝实现了资产损失率小于亿分之0.98的目标。

目前，蚂蚁集团已经开源了TuGraph系统中的单机版图数据TuGraph-DB和流式图计算引擎TuGraph-Analytics。TuGraph-DB提供了完备的图数据库基础功能和成熟的产品设计，具备完整的事务支持和丰富的系统特性，可在单机上部署，使用成本低，支持TB级别的数据规模。而TuGraph-Analytics是业界首个工业级流式图计算引擎，能够在超大规模图上进行流式复杂计算。

未来，蚂蚁将持续加强技术开放，为业界带来先进的图计算技术能力，与行业携手建设图计算技术生态。


蚂蚁图数据库再获LDBC权威测试世界第一
 
近日，国际权威图数据库测试机构国际关联数据基准委员会（LDBC）公布了行业通用的社交网络基准测试（LDBC SNB）最新结果。蚂蚁集团图数据库TuGraph打破官方审计测试纪录，再次拿到世界第一，这一纪录较LDBC早前公布的最高纪录吞吐量提升了52%，也超过了两年前由TuGraph保持的世界纪录1倍以上。
 
据LDBC官方发布的报告，在本次测试中，TuGraph在不同规模的数据集下均表现优异，在最大数据规模300G的数据集（8亿个结点，53亿条边）上，TuGraph的吞吐率较上一次官方纪录提升了52%，在系统事务性、可恢复性、正确性、稳定性等方面均达到官方标准，体现了TuGraph高并发低延迟的强大性能优势。

为了更加贴近真实场景使测试更加严谨，TuGraph还采用了Client/Server部署，将客户端和服务器分别部署在两台服务器上，在更严苛的条件下（固有网络延迟与网络波动）完成了本次测试。

蚂蚁集团也是LDBC最新的金融图数据测试基准Finbench的发起人和主要建设者。

关于LDBC和SNB测试

LDBC，即“关联数据基准测评委员会”（Linked Data Benchmark Council），是全球公认的图数据库领域基准指南制定者与测试机构，与TPC并称为国际数据库行业两大权威技术组织。

SNB，即社交网络基准测试 （Social Network Benchmark），是由LDBC开发的面向图数据库的基准测试（Benchmark）之一。SNB测试由于更贴近现实系统，同时包含了读写任务，简单和复杂查询，规定了系统的响应时间，更能体现系统的综合性能，是目前图数据行业最成熟和通用的性能测试。

LDBC SNB测试由指定的第三方机构进行，从数据导入到结果验证均由第三方在云平台上执行，最终结果由LDBC执行委员会进行审计并公布，最大限度的保证了结果的可信性。同时，SNB还公布了测试过程所用的程序和脚本，以及测试过程中产生的详细结果，进一步确保了测试的可复现性。

关于TuGraph

蚂蚁集团图数据库TuGraph是基于图模型的一站式数据存储和分析系统，擅长处理大规模关联数据的管理和分析，如社交关系、物流服务、设备管网、金融交易等场景，数千倍优化分析性能，天然具备数据可视化展示。

TuGraph拥有业界领先的集群规模和性能，是蚂蚁集团金融风控能力的重要基础设施，显著提升了欺诈洗钱等金融风险的实时识别能力和审理分析效率，提供了稳定的决策支持能力，其中，支撑支付宝的重要风险识别能力提升了近10倍，风险审理分析效率提升90%。
TuGraph 已被成熟应用于安全风控、信贷风控、知识图谱、数据血缘、资金分析、流量归因分析、会员关系等场景，并面向金融、工业、政务服务等行业客户。
 


TuGraph：从清华到蚂蚁
 
蚂蚁集团图计算TuGraph（原名GeaGraph），是蚂蚁集团与清华大学联合研发的大规模图计算系统，构建了一套包含图存储、图计算、图学习、图研发平台的完善的图技术体系，拥有业界领先规模的图集群，是图数据库基准性能测试LDBC-SNB世界纪录保持者。
TuGraph是蚂蚁集团金融风控能力的重要基础设施，显著提升了欺诈洗钱等金融风险的实时识别能力和审理分析效率，并面向金融、工业、政务服务等行业客户。


清华时期

早在2010年前后，清华大学计算机系高性能所就开始图计算相关技术及系统研究。于2016年成功研发的双子星图计算系统比业界常用的开源图计算引擎GraphX性能提高了约100倍，获得了业界的广泛关注。

费马时期

为了推动技术的广泛应用，2016年从事图计算研究的清华师生成立了费马科技有限公司。费马科技在推进双子星系统应用的同时，进一步开发出了具有国际领先性能的图数据库产品TuGraph，能够支持完整的图数据库事务，服务了搜狗搜索、京东金融、以及大型国有银行、国家级能源企业在内的不同行业用户，并在2020年通过了国际图数据库标准组织LDBC的认证测试，认证成绩是第二名的7.6倍。

蚂蚁时期

蚂蚁集团在很多领域具有科技领先能力，国内乃至全球最大的用户量和峰值交易量使得蚂蚁集团对图计算有着丰富的应用需求，利用图计算技术处理支付宝的反欺诈、反套现等难题，可以比传统技术更加适用。从2015年起，蚂蚁集团开始自主研发分布式图数据库、流式图计算等图计算技术系统，并在内部应用中获得了良好的效果。

2020年，蚂蚁集团进一步整合了自有的图计算技术系统GeaBase，以及清华大学和费马科技的产品和技术，升级形成了一套完整的图计算系统GeaGraph（后统一采用TuGraph命名）。这套系统集成了各方原有优势，经过多年技术积累和大规模实战打磨，无论从功能的完整性，还是吞吐率、响应时间等性能指标，都达到了世界领先水平。
 
 


2023开放原子全球开源峰会，蚂蚁图计算平台开源业内首个工业级流图计算引擎

6月11日，2023开放原子全球开源峰会在北京开幕。本次峰会以“开源赋能，普惠未来”为主题。在高峰论坛上，蚂蚁技术研究院院长、图计算负责人陈文光宣布开源 TuGraph 图计算平台核心成员——工业级流式图计算引擎 TuGraph Analytics。

去年9月，蚂蚁集团开源了 TuGraph 图计算平台中的图数据库 TuGraph DB。这次开源是 TuGraph 图计算平台的又一次开源升级，进一步加大了蚂蚁在图计算基础软件领域的开放力度，也是通过开放协同促进科技创新的实际行动。

图（Graph）是一种抽象的数据结构，由顶点和边构成。图计算是一种以图结构建模的算法模型，可对大规模数据进行关系挖掘和复杂计算，实现知识推理和事件溯源。图计算目前已广泛应用在金融、政务、医疗等领域，备受全球研发机构和顶尖科技公司关注。流式图计算是一种将流式计算和图计算结合的交叉创新，融合了流式计算的高度实效性和图计算的灵活性，攻坚难度极高。据了解，蚂蚁从2015年开始探索图计算，布局了图数据库、流式图计算引擎、图学习等相关技术，打造了世界规模领先的图计算集群，于业界首创了工业级流式图计算引擎，多次问鼎图数据库行业权威测试 LDBC 世界冠军并保持世界纪录。此次开源的工业级流式图计算引擎是蚂蚁从2017年开始布局打造，经过五年多工业级应用大考，流式图计算做到了在千亿数据规模的“图”上秒级延迟计算，是蚂蚁风控的核心基础技术，成功解决了金融场景风险分析难、识别率低、时效性差等业界难题。

图计算是下一代人工智能关键核心技术。中国工程院院士郑纬民曾指出，“高性能图计算是当前全球人工智能竞争的战略性制高点，我们要加快攻克技术、突破产业瓶颈，防止在高性能图计算这一关键技术领域再被卡脖子”。而开源是共享科技成果，加速先进技术落地的最快路径。陈文光强调，开源是蚂蚁的核心技术战略，也是面向数字化未来可持续创新的动力。此次流式图计算引擎开源，是延续蚂蚁开源核心基础技术的实际动作，希望通过开放成熟的图计算技术，服务更广阔的数字化产业，向世界输出中国科技公司的前沿技术影响力。未来，蚂蚁集团也愿意携手行业伙伴共同突破技术创新，繁荣开源生态。据了解，蚂蚁开源聚焦于数据库、云原生、中间件等基础软件领域，积累了近100个社区头部开源项目、近1600个开源仓库、9大核心开源项目，如“2022世界人工智能大会镇馆之宝”隐语隐私计算技术栈、分布式数据库 OceanBase、行业首个通过商用密码产品认证的密码学技术“铜锁”等自研核心技术。
 

TuGraph Analytics (别名：GeaFlow) 是蚂蚁集团开源的流图计算引擎，支持万亿级图存储、图表混合处理、实时图计算、交互式图分析等核心能力，目前广泛应用于数仓加速、金融风控、知识图谱以及社交网络等场景。


特性

	•	分布式实时图计算
	•	图表混合处理（SQL+GQL语言）
	•	统一流批图计算
	•	万亿级图原生存储
	•	交互式图分析
	•	高可用和Exactly Once语义
	•	高阶API算子开发
	•	UDF/图算法/Connector插件支持
	•	一站式图研发平台
	•	云原生部署

实时能力

相比传统的流式计算引擎比如Flink、Storm这些以表为模型的实时处理系统而言，GeaFlow以图为数据模型，在处理Join关系运算，尤其是复杂多跳的关系运算如3跳以上的Join、复杂环路查找上具备极大的性能优势。


GeaFlow简介

GeaFlow起源

早期的大数据分析主要以离线处理为主，以Hadoop为代表的技术栈很好的解决了大规模数据的分析问题。然而数据处理的时效性不足， 很难满足高实时需求的场景。以Storm为代表的流式计算引擎的出现则很好的解决了数据实时处理的问题，提高了数据处理的时效性。 然而，Storm本身不提供状态管理的能力， 对于聚合等有状态的计算显得无能为力。Flink 的出现很好的弥补了这一短板，通过引入状态管理以及Checkpoint机制，实现了高效的有状态流计算能力。

随着数据实时处理场景的丰富，尤其是在实时数仓场景下，实时关系运算(即Stream Join) 越来越多的成为数据实时化的难点。Flink虽然具备优秀的状态管理能和出色的性能，然而在处理Join运算，尤其是3度以上Join时， 性能瓶颈越来越明显。由于需要在Join两端存放各个输入的数据状态，当Join变多时，状态的数据量急剧扩大，性能也变的难以接受。 产生这个问题的本质原因是Flink等流计算系统以表作为数据模型，而表模型本身是一个二维结构，不包含关系的定义和关系的存储， 在处理关系运算时只能通过Join运算方式实现，成本很高。

在蚂蚁的大数据应用场景中，尤其是金融风控、实时数仓等场景下，存在大量Join运算，如何提高Join 的时效性和性能成为我们面临的重要挑战，为此我们引入了图模型。图模型是一种以点边结构描述实体关系的数据模型，在图模型里面，点代表实体， 边代表关系，数据存储层面点边存放在一起。因此，图模型天然定义了数据的关系同时存储层面物化了点边关系。基于图模型，我们实现了新一代实时计算 引擎GeaFlow，很好的解决了复杂关系运算实时化的问题。目前GeaFlow已广泛应用于数仓加速、金融风控、知识图谱以及社交网络等场景。

技术架构

GeaFlow整体架构如下所示：
	•	DSL层：即语言层。GeaFlow设计了SQL+GQL的融合分析语言，支持对表模型和图模型统一处理。
	•	Framework层：即框架层。GeaFlow设计了面向Graph和Stream的两套API支持流、批、图融合计算，并实现了基于Cycle的统一分布式调度模型。
	•	State层：即存储层。GeaFlow设计了面向Graph和KV的两套API支持表数据和图数据的混合存储，整体采用了Sharing Nothing的设计，并支持将数据持久化到远程存储。
	•	Console平台：GeaFlow提供了一站式图研发平台，实现了图数据的建模、加工、分析能力，并提供了图作业的运维管控支持。
	•	执行环境：GeaFlow可以运行在多种异构执行环境，如K8S、Ray以及本地模式。
 
应用场景

实时数仓加速

数仓场景存在大量Join运算，在DWD层往往需要将多张表展开成一张大宽表，以加速后续查询。当Join的表数量变多时，传统的实时计算引擎很难 保证Join的时效性和性能，这也成为目前实时数仓领域一个棘手的问题。基于GeaFlow的实时图计算引擎，可以很好的解决这方面的问题。 GeaFlow以图作为数据模型，替代DWD层的宽表，可以实现数据实时构图，同时在查询阶段利用图的点边物化特性，可以极大加速关系运算的查询。
实时归因分析

在信息化的大背景下，对用户行为进行渠道归因和路径分析是流量分析领域中的核心所在。通过实时计算用户的有效行为路径，构建出完整的转化路径，能够快速帮助业务看清楚产品的价值，帮助运营及时调整运营思路。实时归因分析的核心要点是准确性和实效性。准确性要求在成本可控下保证用户行为路径分析的准确性;实效性则要求计算的实时性足够高，才能快速帮助业务决策。 基于GeaFlow流图计算引擎的能力可以很好的满足归因分析的准确性和时效性要求。如下图所示：

GeaFlow首先通过实时构图将用户行为日志转换成用户行为拓扑图，以用户作为图中的点，与其相关的每个行为构建成从该用户指向埋点页面的一条边.然后利用流图计算能力分析提前用户行为子图，在子图上基于归因路径匹配的规则进行匹配计算得出该成交行为相应用户的归因路径，并输出到下游系统。
实时反套现

在信贷风控的场景下，如何进行信用卡反套现是一个典型的风控诉求。基于现有的套现模式分析，可以看到套现是一个环路子图，如何快速，高效在大图中快速判定套现，将极大的增加风险的识别效率。以下图为例，通过将实时交易流、转账流等输入数据源转换成实时交易图，然后根据风控策略对用户交易行为做图特征分析，比如环路检查等特征计算，实时提供给决策和监控平台进行反套现行为判定。通过GeaFlow实时构图和实时图计算能力，可以快速发现套现等异常交易行为，极大降低平台风险。


深入解读TuGraph计算引擎模型推理系统

TuGraph计算引擎模型推理系统将基于迭代计算的图计算框架与模型推理系统相结合，推理系统可自定义推理依赖环境，图迭代计算与推理链路实现隔离。基于共享内存的跨进程通信方式，提高了推理数据交换效率，满足流图近线推理的时效性。在蚂蚁集团内部的实际应用场景中，大幅缩短了模型推理上线的链路与开发时间，用户迭代模型版本更方便快捷。

1. 图算法概述
   
在计算机科学中，图是一种表示实体（节点或顶点）以及实体之间关系（边）的数据结构。图模型可以天然地描述网络结构，能更清晰地表达复杂的数据关系和依赖，简化关联数据的理解和分析。在不同的场景下，图中点边具备不同的语义信息。比如在资金交易场景下，每个人可以抽象成一个点表示，人与人之间的转账关系可以抽象成一条边表示。如下图，通过图数据模型反映出各个实体之间的资金往来关系，让数据的关联分析更加直观和高效。

资金交易图谱示例

在图数据模型上可以执行多种图算法，如社区检测，最短路径匹配，环路检测算法等。通过点边上的迭代计算，探索图模型中各个实体之间的关系。探索过程不依赖于数据的线性结构，从而便于识别隐藏的模式和关联关系。在主流迭代图算法中，节点通过消息传递的方式进行通信。每次迭代，节点可以接收来自它们邻居的消息，处理这些消息，然后决定是否发送新的消息给其他节点。迭代算法中，每个节点有一个状态，每次迭代它们都有可能更新这个状态直至收敛。例如，在PageRank算法中，每个节点的状态是其PageRank值，这个值在迭代过程中会随着邻居的值的更新而更新。

图迭代算法解决了经典的图计算问题，但随着业务需求的复杂度提升，基于迭代的图算法存在着表达能力不足、自适应性能力差、异质图处理难度大等缺点。近年来随着深度学习的研究和应用的发展，以图神经网络（Graph Neural Networks，GNNs）为代表的一类神经网络算法，被设计用来捕获图中实体（节点）和关系（边）间的复杂模式。图神经网络能够结合节点特征和图的结构来学习节点和边的表示，相比之下，传统的迭代图算法通常不会直接从原始特征中学习，而更多地专注于结构特征。依赖于深度学习的天然优势，GNNs具有更强的表示学习能力，可以自动从数据中学习复杂的模式，这使得 GNNs 能够更好地处理多任务学习和迁移学习等问题。在社交网络分析、知识图谱、生物分子网络、推荐系统以及交通网络等领域，得到广泛应用。

2. 流图推理简介
   
TuGraph计算引擎（TuGraph Analytics[1]）是蚂蚁集团开源的大规模分布式实时图计算引擎（流图引擎），实现了流批一体的图计算模型，支持了丰富的图计算算法。TuGraph Analytics的流图计算能力，能处理连续输入的数据流，并支持增量的计算模式，极大得提高了数据的计算效率和实时性。TuGraph Analytics解决了业界大规模数据关联分析的实时计算问题，已广泛应用于数仓加速、金融风控、知识图谱以及社交推荐等场景。

随着业务场景中问题复杂度的提升，基于传统的迭代图算法已无法满足业务的实际需求。例如在反洗钱场景中，利用图神经网络算法处理复杂的交易关系，能够捕获到节点的局部图结构信息。通过聚合邻接节点的特征信息，每个交易节点都可以感知到周边图网络结构的信息。类似的图神经网络等AI模型的推理逻辑，是无法基于传统的图迭代计算模式直接高效地表达的。

受上述问题启发，我们思考是否可以将TuGraph Analytics的流图计算能力与图神经网络等深度学习模型相结合，开发一套基于流图计算的模型推理系统。最终期望的推理系统具备如下能力：

	•	对于图算法工程师，在图迭代计算过程中，能够方便地使用机器学习模型的推理能力。
	•	对于AI算法工程师，可以通过TuGraph Analytics分布式流式计算的能力实现实时的模型推理。
 
众所周知，在深度学习为代表的数据科学领域，Python已经成为数据分析、模型训练和推理框架的主流开发语言，并提供了丰富的开发库和框架生态。而以Hadoop全家桶为代表的大数据计算引擎领域，基于Java语言开发的系统仍占据一席之地，当然TuGraph Analytics也在其中。这种语言差异带来的“互操作性”成本，使得相当一部分大数据和AI生态组件无法轻松地融合，这也是TuGraph Analytics支持图推理需要亟待解决的问题。

3. 系统设计
   
我们对业内的跨Python & Java语言的方案进行了充分的调研，通过深入对比现有的跨语言交互方案的性能与效率，最终决定将模型推理任务运行于Python原生环境中以发挥出最佳的性能。

1. OONX

OONX是一个开发的生态系统，为不同的机器学习框架之间提供一个标准的模型表示格式。它使得开发人员能够在不同的框架、工具、运行时环境之间以一种标准方式交换模型，从而简化了模型的迁移和部署。

优点

框架互操作性

规范化模型格式

优化推理

生态系统支持

缺点

更新滞后

版本兼容性

转换成本高

性能不一致

2. Jython

以Jython为代表的方式，主要思想是在运行的宿主虚拟机上，使用宿主语言重新编写实现。

优点

Java集成

跨平台

线程

缺点

版本管理复杂

支持库有限

更新滞后

支持Python3有限

3. Py4j

Py4j桥接库为代表的方式，以Socket通信模型为基础，实现Python和Java互相访问对象，方法，提供两个程序相互通信的能力。

优点

跨语言交互

动态代理

支持复杂类型

API使用简易

缺点

网络传输

部署分发难度大

版本难兼容

运行时环境依赖复杂

4. Web服务化

Web服务化是一种将机器学习模型部署成网络服务，调用者通过相应的api获取模型推理结果。
优点

扩展性好

简易且轻量

社区支持

机器学习类库易集成

缺点

性能差

不适合计算密集型场景

无状态管理

并发连接有限

在TuGraph Analytics模型推理系统的架构设计中，核心部分是通过C++原生语言建立起来的一座桥梁，实现Python环境和Java虚拟机之间高效的数据交互和操作指令的传递。通过使用C++作为媒介语言，我们不仅能够利用其接近硬件的执行效率，确保数据交互的性能，还能够保证在两个虚拟环境之间数据交换的计算精度和稳定性。基于共享内存的设计允许Python和JVM进程各自独立运行，既保证了运行环境的安全隔离，又能实现数据的高效共享。

4. 技术原理
TuGraph Analytics模型推理系统工作流中，Driver端（即控制节点）发挥着至关重要的角色。该节点运行在Java虚拟机进程，是整个推理流程的控制中心。Driver端初始化了一个非常关键的组件——InferenceContext对象，InferenceContext对象被设计为模型推理流程的核心，在JVM环境中创建并负责加载和预处理用户提供的模型文件和环境依赖信息。在模型推理任务之前，InferenceContext会详细检查并准备好模型文件，确保能够正确加载到预期的执行环境中。InferenceContext也负责初始化和配置与模型推理相关的虚拟环境，确保正确的Python环境或其他必要的运行时库得以安装和配置。

如图所示，由流式数据源源不断的触发图迭代计算与模型推理工作。TuGraph计算引擎提供了DeltaGraphCompute计算接口，用户可自主定义增量图数据的处理逻辑，并更新历史的图存储(Graph Store)。通过TuGraph计算引擎模型推理系统，增量图迭代的中间计算结果，经过推理前置数据处理接口，并基于共享内存的跨进程通信方式，将处理后的数据流输入到推理进程，完成推理工作后的结果参与后续图迭代计算逻辑。下文将详细介绍各个数据接口的使用。

4.1 计算推理隔离

在TuGraph Analytics模型推理系统的架构中，集群的工作负载分配给多个worker节点。每个worker节点上运行着两个关键进程：负责图数据迭代计算的Java进程，以及执行模型推理的Python进程。为了充分利用计算系统的资源，推理进程在没有接收到推理请求时，会进入睡眠状态。这样的设计不仅减少了系统资源的占用，而且降低了系统的整体能耗。当推理请求到达时，推理进程会被立即唤醒，接收和执行新的推理任务。借助睡眠与唤醒机制以及智能的任务调度策略，可以保证系统能够以高效、稳定、节能的方式运行，同时满足了大规模图数据处理和实时推理的需求。

在每个worker工作节点下，按照不同的推理作业级别划分基础的虚拟环境，从而保证一个wroker节点也可以支持不同推理任务，支持标准的requirements.txt管理推理依赖库。

在图迭代计算进程和推理进程之间通过数据队列实现双边数据的交互，通过在数据包的头文件中插入参数个数，长度等信息，推理进程在连续若干次收到空消息包后，将自动进入睡眠状态，释放cpu等资源。图迭代计算进程调用推理接口时，推理进程将快速退出睡眠状态，接收输入数据并完成推理流程。

4.2 跨进程数据交换

对于推理数据的交换部分，底层通过C++开发共享内存管理模块，实现两个进程之间的数据交互。在推理初始化阶段，由InferenceContext对象开辟进程共享内存，Java进程负责创建并初始化推理（Python）进程，通知推理进程共享内存的地址信息，并映射到相应的进程。如图，Java进程和推理进程均采用C++作为桥梁语言，实现共享内存中数据的流动操作。

在推理系统的性能测试阶段，我们发现推理进程读写进程时，接口的调用开销不容忽视。常规的理解认为C++能够优化Python的执行效率，但前提是Python的执行内存足够复杂，优化执行内容的收益远大于接口的调用开销。然而，在我们系统设计中，共享内存的读写接口只是操作了内存地址，实现读写指针的移动。因此，接口的调用开销也是影响推理性能的关键因素，为此，我们充分调研了业界主流的方案。


如图所示，我们对比了多种Python调用C链接库的方案，性能是第一要素，因此选择Cython作为推理进程和底层内存交互的工具。Cython是一个编程语言，是Python语言的一个超集，它将/C++的静态类型系统融合在了Python中，允许开发者可以在Python代码中直接使用C语言的特性，从而提高程序的执行效率。Cython将Python源代码翻译为C或C++代码，然后将其编译为二进制代码，能够显著提高数值计算和循环场景的代码执行性能。

4.3 推理接口设计

上文介绍了采用Cython作为推理进程内存管理的链接工具，如下为TuGraph Analytics模型推理系统的内存管理接口设计，提供了初始化，读和写三个接口。

1. 初始化接口：负责共享内存地址的映射和读指针的初始化。
2. 读接口：数据bytes的长度作为输入参数，直接在内存端上移动相应长度返回数据段，并移动到读指针。
3. 写接口：将bytes和bytes长度写入到共享内存，并移动至写指针。
   
@cython.final cdef class InferIpc:      cdef MmapIPC * ipc_bridge;     cdef uint8_t* read_ptr;      def __cinit__(self, r, w):         self.ipc_bridge = new MmapIPC(r, w)         self.read_ptr = self.ipc_bridge.getReadBufferPtr()      cpdef inline bytes readBytes(self, bytesSize):         if bytesSize == 0:             return b""         cdef int readSize         cdef int len_ = bytesSize         with nogil:             readSize = self.ipc_bridge.readBytes(len_)         if readSize == 0:             return b""         cdef unsigned char * binary_data = self.read_ptr         return binary_data[:len_]      cpdef inline bool writeBytes(self, bytesBuf, length):         cdef bool writeFlag         cdef int len_ = length         cdef char* buf_ = bytesBuf         with nogil:             writeFlag = self.ipc_bridge.writeBytes(buf_, len_)         return writeFlag      def __dealloc__(self):         del self.ipc_bridge

如下为用户实现推理的Java接口，同其它图迭代计算接口一样，需要推理的时候直接调用该接口，将图迭代的中间结果inputs发送到推理进程并返回模型结果。

public interface GraphInferContext<OUT> extends Closeable {     OUT infer(Object... inputs); }

5. 最佳实践

我们以PageRank任务结合群组打分模型推理流程为例，演示具体的操作流程。

5.1 数据处理

定义推理数据前后置处理逻辑如下：

import abc import json import sys import os import torch  class MyInference(TransFormFunction):     def __init__(self):         super().__init__(2)      def transform_pre(self, *args):         return args[0], args[1]      def transform_post(self, *args):         return args[0]

5.2 图迭代推理

定义图迭代计算结合推理逻辑如下：

    public static class PRVertexCentricComputeFunction implements         IncVertexCentricComputeFunction<Integer, Integer, Integer, Integer> {          private IncGraphComputeContext<Integer, Integer, Integer, Integer> graphContext;         private IncGraphInferContext<String> inferContext;          @Override         public void init(IncGraphComputeContext<Integer, Integer, Integer, Integer> graphContext) {             this.graphContext = graphContext;             this.inferContext = (IncGraphInferContext<String>) graphContext;         }          @Override         public void evolve(Integer vertexId,                            TemporaryGraph<Integer, Integer, Integer> temporaryGraph) {             long lastVersionId = 0L;             IVertex<Integer, Integer> vertex = temporaryGraph.getVertex();             HistoricalGraph<Integer, Integer, Integer> historicalGraph = graphContext                 .getHistoricalGraph();             if (vertex == null) {                 vertex = historicalGraph.getSnapShot(lastVersionId).vertex().get();             }              if (vertex != null) {                 List<IEdge<Integer, Integer>> newEs = temporaryGraph.getEdges();                 List<IEdge<Integer, Integer>> oldEs = historicalGraph.getSnapShot(lastVersionId)                     .edges().getOutEdges();                 if (newEs != null) {                     for (IEdge<Integer, Integer> edge : newEs) {                         graphContext.sendMessage(edge.getTargetId(), vertexId);                     }                 }                 if (oldEs != null) {                     for (IEdge<Integer, Integer> edge : oldEs) {                         graphContext.sendMessage(edge.getTargetId(), vertexId);                     }                 }             }          }          @Override         public void compute(Integer vertexId, Iterator<Integer> messageIterator) {             int max = 0;             while (messageIterator.hasNext()) {                 int value = messageIterator.next();                 max = max > value ? max : value;             }             IVertex<Integer, Integer> vertex = graphContext.getTemporaryGraph().getVertex();             IVertex<Integer, Integer> historyVertex = graphContext.getHistoricalGraph().getSnapShot(0).vertex().get();             if (vertex != null && max < vertex.getValue()) {                 max = vertex.getValue();             }             if (historyVertex != null && max < historyVertex.getValue()) {                 max = historyVertex.getValue();             }             graphContext.getTemporaryGraph().updateVertexValue(max);         }          @Override         public void finish(Integer vertexId, MutableGraph<Integer, Integer, Integer> mutableGraph) {             IVertex<Integer, Integer> vertex = graphContext.getTemporaryGraph().getVertex();             List<IEdge<Integer, Integer>> edges = graphContext.getTemporaryGraph().getEdges();             if (vertex != null) {                 mutableGraph.addVertex(0, vertex);                 graphContext.collect(vertex);             } else {                 LOGGER.info("not found vertex {} in temporaryGraph ", vertexId);             }             if (edges != null) {                 edges.stream().forEach(edge -> {                     mutableGraph.addEdge(0, edge);                 });             }             List<String> inferInput = new ArrayList<>();             inferInput.add(String.valueOf(vertexId));             inferInput.add("param2");             String infer = this.inferContext.infer(inferInput.toArray(new Object[]{0}));         }     }
    
5.3 创建作业

在Console作业管理平台创建一个HLA任务，上传图迭代计算jar包，模型文件和依赖管理文件。

5.4 配置参数

配置相关参数，启动运行作业即可。

"geaflow.infer.env.enable":"true", // 初始化虚拟环境等待时间 "geaflow.infer.env.init.timeout.sec":120, // 是否接收日志 "geaflow.infer.env.suppress.log.enable":"true"

6. 总结

通过将AI模型推理引入TuGraph Analytics流图计算系统，让我们能够对图数据进行深度地分析和预测。利用最新的机器学习和深度学习技术，TuGraph Analytics图计算引擎不仅可以对图数据进行分类和回归分析，还可以预测未来趋势，从而在多个维度上提供决策支持。

希望通过以上的介绍，可以让大家对TuGraph Analytics模型推理系统有个比较清晰的了解，非常欢迎大家加入我们社区（https://github.com/TuGraph-family/tugraph-analytics），一起构建图数据上的智能化分析能力！
 
 
Q: TuGraph 的边是否支持索引？

A: TuGraph 在引擎层支持边索引，可通过存储过程使用。Cypher的边索引功能正在开发支持中

Q: TuGraph 单机的QPS是多少？

A: 不同数据规模，不同查询操作的QPS差异较大，比如LDBC SNB典型图操作超过1.2万，参考测试结果：https://www.tugraph.org/blog?id=0

Q: 可视化文件 build 后如何更新到 tugraph 服务？ 

A: 可视化文件打包后，需要进行以下操作进行替换。
	•	登录 tugraph 服务所在的服务或 docker 容器内。
	•	通过 lgraph_server --help 查看服务启动的配置文件所在目录。通常情况：/usr/local/etc/lgraph.json
	•	查看 /usr/local/etc/lgraph.json文件中 web 的配置目录。通常情况：/usr/local/share/lgraph/resource
	•	将可视化打包后生成的文件夹中的内容全部替换到配置目录下 /usr/local/share/lgraph/resource
	•	重新启动 tugraph 服务

Q：如何通过 npm run dev，连接已有的 tugraph 服务？ 

A：启动之前，需要修改文件.env.development中的'VUE_APP_REQUESTURL'的配置项。然后在通过npm run dev进行启动。 示例： NODE_ENV = development VUE_APP_TITLE = TuGraph(dev) VUE_APP_REQUESTURL = http://localhost:7070/


Q：client 目前有哪些编程语言，是否支持 node js？

A：目前主要支持的编程语言有 c++,python,java；目前不支持 node js。使用 node 作为主要开发语言的用户，可以使用 tugraph 提供的 restful api 来调用。建议使用 Cypher 来封装调用接口。后续版本 restful api 将不再进行更新维护，只会保留登录、登出、刷新 token、cypher 调用这几个常见的 api。

Q：python client 是否支持 pip install？client 在哪里进行引用？ 

A：目前 python client 不支持 pip 进行安装。client 在目录https://github.com/TuGraph-db/tugraph-db/tree/master/src/client。


Q: TuGraph 可以对接那些常用数据库？

A: TuGraph通过DataX可以实现大部分主流数据库的导入导出，支持的数据库包括MySQL、Oracle、Hive 等。具体参考https://github.com/TuGraph-db/DataX

Q：如何加载存储过程或算法包？ 

A：加载方式有两种：
	•	第一种：通过可视化页面的插件模块，通过交互操作完成加载。
	•	第二种：通过 cypher 语句实现存储过程的加载。
CALL db.plugin.loadPlugin(plugin_type::STRING,plugin_name::STRING,plugin_content::STRING,code_type::STRING,plugin_description::STRING,read_only::BOOLEAN) :: (::VOID)


Q：如何调用或执行存储过程？ 

A：可以使用 cypher 进行存储过程的执行或调用。
CALL db.plugin.callPlugin(plugin_type::STRING,plugin_name::STRING,param::STRING,timeout::DOUBLE,in_process::BOOLEAN) :: (success::BOOLEAN,result::STRING)

Q:开源内置的算法包在哪里？

A：代码地址https://github.com/TuGraph-db/tugraph-db/tree/master/plugins

Q：如何使用 docker 镜像安装？ 

A：
	•	确认本地是否有 docker 环境，可使用docker -v进行验证。如果没有请安装 docker，安装方式见 docker 官网文档https://docs.docker.com/install/ 。
	•	下载 docker 镜像，下载方式可使用docker pull tugraph/tugraph-runtime-centos7，也可以在官网下载页面进行下载https://www.tugraph.org/download[注：下载的文件是*.tar.gz 的压缩包，不用解压]。
	•	如果使用 docker pull 下载的镜像则不用导入镜像。如果使用官网下载的压缩包，则要使用docker load -i ./tugraph_x.y.z.tar[注：x.y.z 是版本号的代替符，具体数值根据自己下载的版本进行改写]
	•	启动 docker 容器docker run -d -p 7070:7070 -p 9090:9090 --name tugraph_demo tugraph/tugraph-runtime-centos7 lgraph_server[注：具体的镜像名称 tugraph/tugraph-runtime-centos7 要以本地实际镜像名称为准，可用过 docker images 命令查看]


Q：rpm 包和 deb 包安装后，启动 lgraph_server 服务。提示缺少'liblgraph.so'报错？ 

A：此问题主要是环境变量导致，需要配置环境量。 示例：
export LD_LIBRARY_PATH=/usr/local/lib64


Q：是否支持不定长边的条件查询？ 示例：
 MATCH p=(v)-[e:acted_in|:rate*1..3]-(v2) WHERE id(v) IN [3937] AND e.stars = 3 RETURN p LIMIT 100

A：目前还不支持不定长边的过滤查询。目前的代替方案只能是分开写。上面的示例，就需要从 1 跳到 3 跳都写一遍。


Q：如何查询最短路径，shortestPath 函数如何使用？

A：使用示例如下（示例图谱：MovieDemo）
 MATCH (n1 {name:'Corin Redgrave'}),(n2 {name:'Liam Neeson'})
     CALL algo.allShortestPaths(n1,n2) YIELD nodeIds,relationshipIds,cost
         RETURN nodeIds,relationshipIds,cost

详尽使用方案请参考官网文档https://www.tugraph.org/doc?version=V3.3.0&id=10000000000658658。


Q：查询语句 Where 后使用 and 进行拼接查询速度较慢，语句应如何优化改进？ 示例：
 MATCH (n1),(n2) CALL algo.allShortestPaths(n1,n2)
     YIELD nodeIds,relationshipIds,cost
         WHERE id(n1) IN [0] AND id(n2) IN [3938]
             RETURN nodeIds,relationshipIds,cost

A：目前 cypher 查询引擎正在优化中。现阶段语句改写可以通过 with 向下传递进行优化。 示例：
 MATCH (n1) where id(n1) in [0] with n1
 MATCH (n2) where id(n2) in [3938] with n1, n2
     CALL algo.allShortestPaths(n1,n2) YIELD nodeIds,relationshipIds,cost
         RETURN nodeIds,relationshipIds,cost


Q：如何查询任意跳的边？ 

A：使用*.. 示例：
 MATCH p=(a)-[*..]-(b) WHERE id(a) IN [3] AND id(b) IN [19] RETURN p



Q：报错"User has reached the maximum number of tokens"后，怎么做？ 

A：这表明当前账号Token数量已达上限10000个。解决方法如下，任选其一：
	1	登出不使用的Token。
	2	重新启动TuGraph服务，会清空所有Token。
	3	Token有效期默认为24小时，24小时后会自动失效并删除。


社区贡献 | 新Feature发布：TuGraph-DB支持空间数据类型

本文介绍了来自北京大学的TuGraph开源社区开发者孙同学的工作，孙同学基于开源图数据库TuGraph-DB的开发，支持了空间数据类型。文章将从空间数据类型的应用、在TuGraph-DB中的实现以及使用三个方面进行介绍。孙同学的工作丰富了TuGraph-DB功能，因此获得“TuGraph社区年度优秀贡献者”，再次感谢孙同学的贡献，项目功能正在持续完善中，也欢迎更多伙伴加入TuGraph社区一起共建。

近年来，地理数据类型/空间数据类型（Spatial Data Type）在图数据库中的应用价值显著，它不仅增强了数据的表达能力，还促进了跨领域数据的融合分析，尤其在社交网络、地图探索、城市规划等关键领域展现了强大的实用价值。

空间数据类型在图数据库中的应用案例

空间地理数据也是天然适合使用图数据库的一种场景，我们先来看看在图数据库中使用空间数据的几个经典场景:

场景案例一：判断某空间类型内的坐标

如上图1和图2所示，日常生活中我们通常需要在地图中查询距离自己一定距离内的景点/美食信息。对应的在图数据库中，即为判断哪些坐标在以某点为中心的圆形或矩形区域内，对应的Cypher查询语句如下:

# 判断在圆形区域内

WITH point({latitude: $latitude, longitude:$longitude}) AS radiusCenter

MATCH (p:Point)-[:HAS_GEOMETRY]-(poi:PointOfInterest)-[:HAS_TAGS]->(t:Tags)

    WHERE point.distance(p.location, radiusCenter) < $radius
    
RETURN p {

    latitude: p.location.latitude,
    
    longitude: p.location.longitude,
    
    name: poi.name,
    
    categories: labels(poi),
    
    tags: t{.*}                       
    
} AS point

# 判断在矩形区域内

MATCH (p:Point)-[:HAS_GEOMETRY]-(poi:PointOfInterest)-[:HAS_TAGS]->(t:Tags)

   WHERE point.withinBBox(
   
        p.location,
	
       point({longitude: $lowerLeftLon, latitude: $lowerLeftLat}),
       
       point({longitude: $upperRightLon, latitude: $upperRightLat})
       
   )
   
RETURN p {

  latitude: p.location.latitude,

  longitude: p.location.longitude,
  
  name: poi.name,
  
  categories: labels(poi),
  
  tags: t{.*}
  
} AS point

场景案例二：判断路径与某空间类型是否重合

如上图3所示，移动物体的行动轨迹可以被抽象成一条线，这条轨迹往往伴随着时序信息。日常生活中，有时候我们想知道哪部分行动轨迹和给定区域有重合。对应在图数据库中，即判断一系列坐标中哪些坐标落在在空间类型内。对应的Cypher查询语句如下:

WITH point({latitude: $latitude, longitude: $longitude}) AS radiusCenter

MATCH (g: Geometry)

    WHERE any(
    
        p IN g.coordiates WHERE point.distance(p, radiusCenter) < $radius
	
    )
    
RETURN [n IN g.coordinates | [n.latitude, n.longitude]] AS route

空间数据类型在TuGraph-DB中的实现

需求分析

结合上述案例，我们可以分析总结出对空间数据类型的需求:

• 支持不同坐标系下（包括地球地理坐标系，平面几何坐标系等）不同空间数据类型（包括Point、LineString,、Polygon）的存储与创建

• 支持不同坐标系下的常见空间查询操作, 包括Distance、BoundingBox、Disjoint（判断两个数据是否相交）的查询等

• 支持空间数据索引（R-Tree）

• 支持常见空间数据格式的导入（ESRI Shapefile data / OpenStreetMap）

• 支持空间数据的可视化

空间数据类型的表示

空间数据类型可以用不同的坐标系来表示，EPSG[1]是一个标准化的地理空间参考系统标识符集合， 用于标识不同的地理空间参考系统，包括坐标系统、地理坐标系、投影坐标系等。通常使用EPSG编码表示数据的坐标系。行业内一般采用

• WGS84坐标系（没错，就是GPS系统的坐标系），标识符为EPSG 4326

• Cartesian（笛卡尔）坐标系（没错，就是你高中数学学的直角坐标系），标识符为EPSG 7203

WGS84是全球定位系统(GPS)的基础，允许全球的GPS接收器确定精确位置。几乎所有现代GPS设备都是基于WGS84坐标系来提供位置信息。在地图制作和GIS（地图制作和地理信息系统）领域，WGS84被广泛用于定义地球上的位置。这包括各种类型的地图创建、空间数据分析和管理等。

Cartesian（笛卡尔）坐标系，又称直角坐标系，是一种最基本、最广泛应用的坐标系统。它通过两条数轴定义一个平面，三条数轴定义一个空间，这些轴互相垂直，在数学、物理、工程、天文和许多其他领域中有着广泛的应用。

空间数据类型的实现

OGC(Open Geospatial Consortium) 定义了空间数据的标准表示格式，分别为EWKT(extended well known text)与EWKB(extended well known binary)格式，用于在不同系统和平台之间交换和存储空间数据，现已被广泛采用。

EWKT

EWKT格式数据如下所示，先指定空间数据类型，再在括号内指定具体的坐标，一个坐标对表示一个点，每个坐标对之间用逗号隔开。其中，对于Polygon类型的数据，第一个坐标对需要与最后一个坐标对相同，形成闭合的面。

SRID=s;POINT (<x> <y>)

SRID=s;LINESTRING(<x1> <y1>, <x2> <y2>, …)

注: SRID默认为4326, 可以不指定

EWKB

EWKB格式数据如下图所示，

第0-1位: 表示编码方式 00表示大端法，01表示小端法

• 第2 - 5位: 空间数据类型

• 0100: point

• 0200: linestring

• 0300: polygon

• 第6 - 9位: 数据维度

• 0020: 二维

• 0030: 三维

• 第10 - 17位: 坐标系的EPSG编码

• 第18 - n位: double类型的坐标对的16进制表示

注: 对于POINT类型，其EWKB格式为定长存储，固定长度为50，而对于其他类型，则为不定长。

实现思路

在TuGraph-DB的实现，基于boost geometry库的基础上进行封装，用EWKB格式存储数据，其中Point类型为定长存储50，其余皆为变长存储。我们支持了Point, Linestring与Polygon三种类型，同时支持了WGS84, CARTESIAN两种坐标系，数据类型与坐标系均可根据需要拓展。

在TuGraph-DB中使用空间数据类型

定义空间数据类型

TuGraph-DB当前已经支持Point、Linestring与Polygon三种类型

• Point：点，创建方式例如POINT(2.0, 2.0, 7203)

• Linestring：折线，创建方式例如LINESTRING(0 2,1 1,2 0)

• Polygon：多边形，创建方式例如POLYGON((0 0,0 7,4 2,2 0,0 0))

其中坐标点都是double型

相关函数介绍

创建空间数据相关函数，以Point为例：

| 函数名      | 描述                              | 输入参数                                 | 返回值类型 |
|-------------|-----------------------------------|------------------------------------------|------------|
| `Point()`   | 根据坐标或EWKB创建Point            | 坐标对 (double, double) / EWKB format(string) | point      |
| `PointWKB()`| 根据WKB与指定SRID创建Point         | WKB format(string), SRID(int)            | point      |
| `PointWKT()`| 根据WKT与指定SRID创建Point         | WKT format(string), SRID(int)            | point      |

查询用相关函数：

| 函数名      | 描述                              | 输入参数                                 | 返回值类型 |
|-------------|-----------------------------------|------------------------------------------|------------|
| `Distance()`   | 计算两个空间数据间的距离	           |                                        |            |
| 注：要求坐标系相同| Spatial data1, Spatial data2         | double	            |      |
| `Disjoint()`| 判断两个空间数据是否相交	         |           |     |
| 注：开发中| Spatial data1, Spatial data2        | bool	         |     |
| `WithinBBox()`| 判断某个空间数据是否在给定的长方形区域内	       |        |   |
| 注：开发中| Spatial data, Point1       | bool	         |       |


使用实例如下：

# 创建包含空间数据类型的点模型

CALL db.createVertexLabel('food', 'id', 'id', int64, false, 'name', string, true,'pointTest',point,true) 

# 插入标记美食点的数据

CREATE (n:food {id:10001, name: 'aco Bell',pointTest:point(3.0,4.0,7203)}) RETURN n

# 创建具有折线属性的点模型

CALL db.createVertexLabel('lineTest', 'id', 'id', int64, false, 'name', string, true,'linestringTest',linestring,true)

# 插入具有折线属性的点数据

CREATE (n:lineTest {id:102, name: 'Tom',linestringTest:linestringwkt('LINESTRING(0 2,1 1,2 0)', 7203)}) RETURN n

# 创建具有多边型属性的点模型

CALL db.createVertexLabel('polygonTest', 'id', 'id', int64, false, 'name', string, true,'polygonTest',polygon,true)

# 插入具有多边型属性的点数据

CREATE (n:polygonTest {id:103, name: 'polygonTest',polygonTest:polygonwkt('POLYGON((0 0,0 7,4 2,2 0,0 0))', 7203)}) RETURN n

Demo演示：基于地理位置个性化推荐的美食探索

在本章节中，我们将探索如何利用TuGraph-DB图数据库的地理空间功能，构建一个生动有趣的美食探索应用，“人”与“美食”通过地理位置紧密相连，实现个性化美食推荐。

数据模型设计

定义两种核心节点类型：

• Food（美食）点：每一家餐厅或小吃店都可以作为一个Food节点，其属性包括但不限于名称、地址、评分、美食类别等。特别地，我们将在每个Food节点上附加地理坐标信息，用以精确记录其地理位置。

• Person（人物）点：代表应用的用户，属性包含用户名、当前位置等。用户的当前位置同样通过地理坐标表示，便于后续的地理空间查询。

# 创建Schema

CALL db.createVertexLabel('food', 'id', 'id', int64, false, 'name', string, true,'pointTest',point,true,'mark',double,true)

CALL db.createVertexLabel('person', 'id', 'id', int64, false, 'name', string, true,'pointTest',point,true)

# 插入示例数据 food

CREATE (n:food {id:10001, name: 'Starbucks',pointTest:point(1.0,1.0,7203),mark:4.8}) RETURN n

CREATE (n:food {id:10002, name: 'KFC',pointTest:point(2.0,1.0,7203),mark:4.5}) RETURN n

CREATE (n:food {id:10003, name: 'Pizza Hut',pointTest:point(2.0,5.0,7203),mark:4.5}) RETURN n

CREATE (n:food {id:10004, name: 'Taco Bell',pointTest:point(3.0,4.0,7203),mark:4.7}) RETURN n

CREATE (n:food {id:10005, name: 'Pizza Fusion',pointTest:point(5.0,3.0,7203),mark:4.9}) RETURN n

CREATE (n:food {id:10006, name: 'HaiDiLao Hot Pot',pointTest:point(2.0,2.0,7203),mark:4.8}) RETURN n

CREATE (n:food {id:10007, name: 'Lao Sze Chuan',pointTest:point(4.0,3.0,7203),mark:4.7}) RETURN n

# 插入示例数据 person

CREATE (n:person {id:1, name: 'Tom',pointTest:point(3.0,3.0,7203)}) RETURN n

构建美食探索查询

能够根据用户的当前位置，寻找距离2.5以内的美食，根据距离进行升序排列，返回距离和评分。

MATCH (n:person{id:1}), (m:food)

WITH n.pointTest AS p1, m.pointTest AS p2, m.name AS food, m.mark AS mark

CALL spatial.distance(p1,p2) YIELD distance 

WHERE distance < 2.5

RETURN food,distance,mark

ORDER BY distance

此查询首先匹配特定的Person节点，然后找到所有Food节点，利用自定义的distance函数计算Person节点当前位置与每个Food节点之间的直线距离，筛选出距离在2.5之内的美食。最后，按照美食的距离升序排列结果，附带评分参考，为用户提供最优质的推荐。

未来展望

孙同学的工作，让TuGraph-DB具备了处理地理空间数据的能力。未来，TuGraph-DB将来会继续实现Disjoint() 、WithinBBox()等更多的函数，以及实现更高级的索引、数据导入、可视化等功能，丰富更多的使用场景。作为开源项目，TuGraph社区也欢迎大家一起参与共建开发地理空间功能。


一张图读懂 TuGraph Analytics 开源技术架构

TuGraph Analytics（内部项目名GeaFlow）是蚂蚁集团开源的分布式实时图计算引擎，即流式图计算。通过SQL+GQL融合分析语言对表模型和图模型进行统一处理，实现了流、批、图一体化计算，并支持了Exactly Once语义、高可用以及一站式图研发平台等生产化能力。

开源项目代码目前托管在GitHub，欢迎业界同仁、大数据/图计算技术爱好者关注我们的项目并参与共建。

概览

本文希望通过一张图描述清楚TuGraph Analytics的整体架构脉络和关键设计思路，以帮助大家快速对TuGraph Analytics项目的轮廓有个整体的认识。闲言少叙，直接上图。

TuGraph Analytics开源技术架构一共分为五个部分：

• DSL层：即语言层。TuGraph Analytics设计了SQL+GQL的融合分析语言，支持对表模型和图模型统一处理。

• Framework层：即框架层。TuGraph Analytics设计了面向Graph和Stream的两套API支持流、批、图融合计算，并实现了基于Cycle的统一分布式调度模型。

• State层：即存储层。TuGraph Analytics设计了面向Graph和KV的两套API支持表数据和图数据的混合存储，整体采用了Sharing Nothing的设计，并支持将数据持久化到远程存储。

• Console平台：TuGraph Analytics提供了一站式图研发平台，实现了图数据的建模、加工、分析能力，并提供了图作业的运维管控支持。

• 执行环境：TuGraph Analytics可以运行在多种异构执行环境，如K8S、Ray以及本地模式。

DSL层

DSL层是一个典型的编译器技术架构，即语法分析、语义分析、中间代码生成(IR)、代码优化、目标代码生成（OBJ）的流程。

• 语言设计：TuGraph Analytics设计了SQL+GQL的融合语法，解决了图+表一体化分析的诉求。具体语法设计可以参考DSL语法文档[2]

• 语法分析：通过扩展Calcite的SqlNode和SqlOperator，实现SQL+GQL的语法解析器，生成统一的语法树信息。

• 语义分析：通过扩展Calcite的Scope和Namespace，实现自定义Validator，对语法树进行约束语义检查。

• 中间代码生成：通过扩展Calcite的RelNode，实现图上的Logical RelNode，用于GQL语法的中间表示。

• 代码优化：优化器实现了大量的优化规则（RBO）用于提升执行性能，未来也会引入CBO。

• 目标代码生成：代码生成器Converter负责将Logical RelNode转换为Physical RelNode，即目标代码。Physical RelNode可以直接翻译为Graph/Table上的API调用。

• 自定义函数: TuGraph Analytics提供了大量的内置系统函数，用户也可以根据需要注册自定义函数。

• 自定义插件: TuGraph Analytics允许用户扩展自己的Connector类型，以支持不同的数据源和数据格式。

Framework层

Framework层设计与Flink/Spark等同类大数据计算引擎有一定的相似性，即提供了类FlumeJava（FlumeJava: Easy, Efficient Data-Parallel Pipelines[3]）的统一高阶API（简称HLA），用户调用高阶API的过程会被转换为逻辑执行计划，逻辑执行计划执行一定的优化（如ChainCombine、UnionPushUp等）后，被转换为物理执行计划，物理执行计划会被调度器分发到分布式Worker上执行，最终Worker会回调用户传递的高阶API函数逻辑，实现整个分布式计算链路的执行。

• 高阶API：TuGraph Analytics通过Environment接口适配异构的分布式执行环境（K8S、Ray、Local），使用Pipeline封装了用户的数据处理流程，使用Window抽象统一了流处理（无界Window）和批处理（有界Window）。Graph接口提供了静态图和动态图（流图）上的计算API，如append/snapshot/compute/traversal等，Stream接口提供了统一流批处理API，如map/reduce/join/keyBy等。

• 逻辑执行计划：逻辑执行计划信息统一封装在PipelineGraph对象内，将高阶API对应的算子（Operator）组织在DAG中，算子一共分为5大类：SourceOperator对应数据源加载、OneInputOperator/TwoInputOperator对应传统的数据处理、IteratorOperator对应静态/动态图计算。DAG中的点（PipelineVertex）记录了算子（Operator）的关键信息，如类型、并发度、算子函数等信息，边（PipelineEdge）则记录了数据shuffle的关键信息，如Partition规则（forward/broadcast/key等）、编解码器等。

• 物理执行计划：物理执行计划信息统一封装在ExecutionGraph对象内，并支持二级嵌套结构，以尽可能将可以流水线执行的子图（ExecutionVertexGroup）结构统一调度。图中示例的物理执行计划DAG被划分为三部分子图结构分别执行。

• 调度器：TuGraph Analytics设计了基于Cycle的调度器（CycleScheduler）实现对流、批、图的统一调度，调度过程通过事件驱动模型触发。物理执行计划中的每部分子图都会被转换为一个ExecutionCycle对象，调度器会向Cycle的头结点（Head）发送Event，并接收Cycle尾结点（Tail）的发回的Event，形成一个完整的调度闭环。对于流处理，每一轮Cycle调度会完成一个Window的数据的处理，并会一直不停地执行下去。对于批处理，整个Cycle调度仅执行一轮。对于图处理，每一轮Cycle调度会完成一次图计算迭代。

• 运行时组件：TuGraph Analytics运行时会拉起Client、Master、Driver、Container组件。当Client提交Pipeline给Driver后，会触发执行计划构建、分配Task（ResourceManagement提供资源）和调度。每个Container内可以运行多个Worker组件，不同Worker组件之间通过Shuffle模块交换数据，所有的Worker都需要定期向Master上报心跳（HeartbeatManagement），并向时序数据库上报运行时指标信息。另外TuGraph Analytics运行时也提供了故障容忍机制（FailOver），以便在异常/中断后能继续执行。

State层

State层设计相比于传统的大数据计算引擎，除了提供面向表数据的KV存储抽象，也支持了面向图数据的Graph存储抽象，以更好地支持面向图模型的IO性能优化。

• State API：提供了面向KV存储API，如get/put/delete等。以及面向图存储的API，如V/E/VE，以及点/边的add/update/delete等。

• State执行层：通过KeyGroup的设计实现数据的Sharding和扩缩容能力，Accessor提供了面向不同读写策略和数据模型的IO抽象，StateOperator抽象了存储层SPI，如finish（刷盘）、archive（Checkpoint）、compact（压缩）、recover（恢复）等。另外，State提供了多种PushDown优化以加速IO访问效率。通过自定义内存管理和面向属性的二级索引也会提供大量的存储访问优化手段。

• Store层：TuGraph Analytics支持了多种存储系统类型，并通过StoreContext封装了Schema、序列化器，以及数据版本信息。

• 持久化层：State的数据支持持久化到远程存储系统，如HDFS、OSS、S3等。

Console平台

Console平台提供了一站式图研发、运维的平台能力，同时为引擎运行时提供元数据（Catalog）服务。

• 标准化API：平台提供了标准化的RESTful API和认证机制，同时支持了页面端和应用端的统一API服务能力。

• 任务研发：平台支持“关系-实体-属性”的图数据建模。基于字段映射配置，可以定义图数据传输任务，包括数据集成（Import）和数据分发（Export）。基于图表模型的图数据加工任务支持多样化的计算场景，如Traversal、Compute、Mining等。基于数据加速器的图数据服务，提供了多协议的实时分析能力，支持BI、可视化分析工具的接入集成。

• 构建提交：平台通过任务和作业的独立抽象，实现研发态与运维态的分离。任务开发完成后执行发布动作，会自动触发构建流水线（Release Builder），生成发布版本。任务提交器（Task Submitter）负责将发布版本的内容提交到执行环境，生成计算作业。

• 作业运维：作业属于任务的运行态，平台提供了作业的操纵（启停、重置）、监控（指标、告警、审计）、调优（诊断、伸缩、调参）、调度等运维能力。作业的运行时资源会由资源池统一分配和管理。

• 元数据服务：平台同时承载了引擎运行时的元数据服务能力，以实现研发与运维的自动化。元数据以实例维度进行隔离，实例内的研发资源可以根据名字直接访问，如点、边、图、表、视图、函数等。

• 系统管理：平台提供了多租户隔离机制、细粒度用户权限控制，以及系统资源的管理能力。

执行环境

TuGraph Analytics支持多种异构环境执行，以常见的K8S部署环境为例，其物理部署架构如下：

在TuGraph Analytics作业的全生命周期过程中，涉及的关键数据流程有：

• 研发阶段：Console平台提供了实例下所有的研发资源的管理，用户可以在创建任务前，提前准备所需的研发资源信息，并存储在Catalog。

• 构建阶段：任务创建完成后，通过发布动作触发构建流水线，用户的JAR包、任务的ZIP包等会上传到RemoteFileStore。

• 提交阶段：作业提交时，Console会根据作业的参数配置、运行时环境信息，以及远程文件地址等创建KubernetesJobClient，既而会拉起Client Pod，Client会拉起Master Pod，Master会拉起Container Pods和Driver Pod。所有的Pod拉起后，Client会把作业的Pipeline发送给Driver执行，Driver最终通过Cycle调度的Events与Containers交互。所有的Pod启动时都会从RemoteFileStore下载版本JAR包、用户JAR包、作业ZIP包等信息。Driver对DSL代码编译时，也需要通过Console提供的Catalog API操作Schema信息。

• 运行阶段：作业运行时，各个组件会上报不同的数据和信息。Master会上报作业的心跳汇总信息，Driver会上报作业的Pipeline/Cycle指标以及错误信息，Container会上报作业的Offset、指标定义以及错误信息等。RuntimeMetaStore存储作业的Pipeline/Cycle指标、Offset、心跳汇总、错误等信息。HAMetaStore存储各个运行组件的地址信息。DataStore存储State数据和作业FailOver时所需的元数据信息。MetricStore存储运行时指标信息。

• 监控阶段：Console会主要查询RuntimeMetaStore和MetricStore存储的信息用于作业的运行时监控。

• 清理阶段：作业重置/删除时，Console会对作业的RuntimeMeta、HAMeta以及部分Data做清理操作。

总结

希望通过以上的介绍，可以让大家对TuGraph Analytics开源技术架构有个比较清晰的了解，我们非常欢迎开源社区的技术爱好者参与到项目的建设中来。如果您对TuGraph Analytics项目比较感兴趣，欢迎动动手指扫码直达GitHub仓库，为我们的项目加一颗Star。（网络不畅可以尝试使用VPN访问）

蚂蚁集团开源图数据库TuGraph，成立图计算开源委员会

9月1日，2022世界人工智能大会“新一代图智能技术发展与实践论坛”上，蚂蚁集团图计算负责人陈文光宣布开源蚂蚁集团高性能图数据库TuGraph单机版，并成立图计算开源技术委员会，中国工程院院士郑纬民、陈纯分别担任主席、副主席，5位业界知名专家担任委员。

TuGraph由蚂蚁集团和清华大学共同研发，是图数据库权威测试世界纪录保持者，也是世界上有测试纪录的“最快”的图数据库。

随着TuGraph的开源，图数据领域将迎来一款性能卓越、功能丰富、生态完备的开源产品。

开发者可以聚焦应用层，轻松打造属于自己的图数据，从而提升行业整体技术应用水位。TuGraph开源采用Apache2.0协议，在Github和Gitee上进行托管。


图数据库区别于关系型数据库，基于图模型，使用点边来表示、存储、处理数据，拥有灵活的数据抽象模型，能够更好地表达出“关系”的概念。


蚂蚁TuGraph是一套分布式图数据库系统，可以支持万亿级边上的实时查询。此次开源的TuGraph单机版，同样具备完备的图数据库基础功能和成熟的产品设计，可以轻松支持 TB 级别数据和百亿级别大图，足以满足大多数业务场景需求。相较于市场上常见的开源产品，TuGraph单机版的性能高 10 倍以上。

蚂蚁集团2015年开始自主研发分布式图数据库、流式图计算等图相关技术，2016 年发布自研分布式图数据库，并应用于支付宝。至今TuGraph已应用于蚂蚁内部150多个场景，包括在线支付的实时链路，以支付宝风险识别能力提升近 10倍、风险审理分析效率提升 90%的成绩，验证了其高可靠性。


就在上个月，LDBC（关联数据基准委员会）发布最新图数据库SNB测试结果，TuGraph在功能完整性、吞吐率、响应速度等层面全球领先。

目前，蚂蚁集团已形成了一套以图数据库为底座、同时包含流式图计算，离线图学习的大规模图计算系统。


蚂蚁集团图数据库负责人洪春涛表示，图技术是未来大数据、人工智能和高性能计算产业发展的关键所在，它很有可能会成为下一代的数据底座。蚂蚁集团愿意通过开源持续输出核心技术优势，推动图数据库更广泛的应用生态，携手行业抢占技术高地，不断探索技术的可能性。


推动图数据库查询语言标准化进程，TuGraph 开源 ISO GQL Antlr4 语法文件

TuGraph 团队开源了基于最新的图数据库标准查询语言 ISO GQL（草案）实现的 Antlr4 语法文件，并对表达式等语法定义进行了大量重构，大幅提升了 Antlr4 解析阶段的性能。使用者可以通过 Antlr4 快速接入到现有的图数据库产品之中，快速适配和体验 ISO GQL 最新的语法能力。

GitHub代码仓库：https://github.com/TuGraph-family/gql-grammar

在图数据库领域，通常使用 Antlr4 将不同开发语言的语法结构映射到图数据库中的节点和边，转化为图数据。我们基于2023年3月版本草案中的语法规范，编写了 ISO GQL 的 Antlr4 语法文件，并进行了大量的优化工作，提高了 Antlr4 解析 ISO GQL 语句的性能。Antlr4已在蚂蚁集团内部系统和业务中实际投入使用，平均可以提升解析速度数十倍，在一些复杂查询语句场景下，甚至可以提升至上百倍。

为了降低现有的图数据库快速接入图数据库标准查询语言 ISO GQL 的门槛，推进共建 ISO GQL 标准化的进程，TuGraph 团队于近期开源了最新版本 ISO GQL（2023.3版本）的 Antlr4 语法文件。现有的图数据库及其他系统能以此开源语法文件为基础，更快、更准确地在查询引擎中增加对 ISO GQL 语句的解析与支持。我们期待越来越多的图数据库接入 ISO GQL，共同促进图数据库查询语言的应用与发展。

未来，我们会持续跟进 ISO GQL 的发展，及时更新迭代我们的语法文件，发布更多关于 ISO GQL 的实际使用案例和文档，推动 ISO GQL 在更多产品和业务场景中的使用。

关于 Antlr4

Antlr4 是一款备受欢迎的语法分析器生成工具，能够根据语法规则快速生成自定义解析器。其支持 LL(*) 解析，拥有更强大的错误处理能力和更快的解析速度。不仅如此，Antlr4 还支持 Java、Python、C++、JavaScript、Go 等10种目标语言，广泛应用于多种开发语言生态中。简单易用的 API 和文档使得开发人员能够快速上手。无论是编程语言、数据格式、编译器还是解释器等领域，Antlr4 都发挥着重要作用。著名的开源项目如 Apache Spark、Eclipse IDE 和 MongoDB 等都使用了 Antlr4 进行语法分析。

关于ISO GQL

ISO GQL（ISO/IEC 39075）是一种标准化的图数据库查询语言，由监管 SQL 标准的同一个国际工作组开发和维护，同时也是自 SQL 之后的第一个 ISO/IEC 国际标准数据库语言项目。ISO GQL 图查询语言将结构化数据建模为图，并在图数据库或其他图存储中存储、查询和修改这些数据。目前 ISO GQL 处于草案阶段，蚂蚁集团也是其主要贡献者之一。

TuGraph-Analytics 图计算快速上手之 PageRank 算法


-TuGraph-Analytics 介绍-

TuGraph-Analytics（下文简称GeaFlow）是蚂蚁集团开源的分布式实时图计算引擎，目前广泛应用于金融风控、社交网络、知识图谱以及数据应用等场景。GeaFlow的核心能力是流式图计算，流式图计算相比离线图计算提供了一种高时效性低延迟的图计算模式，更多详细内容参考GitHub介绍(https://github.com/TuGraph-family/tugraph-analytics).

GeaFlow整体架构如下所示：

GeaFlow整体架构从上往下包含以下几层：

GeaFlow DSL GeaFlow对用户提供图表融合分析语言，采用SQL + ISO/GQL方式.用户可以通过类似SQL编程的方式编写实时图计算任务。

GraphView API GeaFlow以GraphView为核心定义的一套图计算的编程接口,包含图构建、图计算以及Stream API接口。

GeaFlow Runtime GeaFlow运行时，包含GeaFlow图表算子、task调度、failover以及shuffle等核心功能。

GeaFlow State GeaFlow的图状态存储，用于存储图的点边数据.同时流式计算的状态如聚合状态也存放在State中。

K8S Deployment GeaFlow支持K8S的方式进行部署运行。

GeaFlow Console GeaFlow的管控平台，包含作业管理、元数据管理等功能。

-PageRank算法介绍-

PageRank是图计算领域一个应用广泛的算法，由Google公司创始人之一拉里·佩奇与谢尔盖·布林在1998年发明，主要用于网页的排序。该算法基于网页之间相互引用的关系，将网页评分的思想引入到搜索引擎中，用于计算网页的重要度和排名。

PageRank算法的核心思想是：一个网页的重要度是由其他网页对它的引用数量和质量决定的。如果一个网页被其他网页引用得多，那么它的重要度就越高。同时，如果一个网页的引用来源也很重要，那么它对被引用网页的贡献也会更大。

实现PageRank算法的具体步骤包括：首先构建网页之间的链接关系图，然后对图进行迭代计算，直到收敛为止。在每一次迭代中，每个网页的得分都会被重新计算，并更新到下一次迭代中。最后，按照网页得分的大小对搜索结果进行排序，输出排名前几位的网页。如下有4个页面，A, B, C, D：

以A点为例，其每一轮的PageRank值计算方法如下：

PR(A) = d * (PR(D)/ 2 + PR(B)/1 + PR(C)/2) + (1- d)

每一个点的PageRank值等于其入点的PageRank值除以入点出边数的加权和，其中d为0~1之间的修正系数。

PageRank算法在搜索引擎中广泛应用，成为搜索引擎排名的重要算法之一。除此之外，PageRank算法的思想也在社交网络、推荐系统等领域得到了应用。


-TuGraph-Analytics 实现 PageRank-

接口与实现

TuGraph-Analytics支持在图查询里调用图算法，语法形式如下：

INSERT INTO tbl_result

CALL page_rank() YIELD (vid, prValue)

RETURN vid, prValue;

我们通过CALL语句调用具体的算法，通过YIELD定义算法的返回字段，比如page_rank算法返回点id和page rank值两个字段，则可以通过YIELD(vid, prValue)来表示。

DSL里面实现一个图算法需要实现AlgorithmUserFunction接口，其定义如下：

/**

 * Interface for the User Defined Graph Algorithm.
 * 
 * @param <K> The id type for vertex.
 * 
 * @param <M> The message type for message send between vertices.
 * 

 */
public interface AlgorithmUserFunction<K, M> extends Serializable {

    /**
    
     * Init method for the function
     
     * @param context The runtime context.
     
     * @param params  The parameters for the function.
     
     */
     
    void init(AlgorithmRuntimeContext<K, M> context, Object[] params);

    /**
    
     * Processing method for each vertex and the messages it received.
     
     */
     
    void process(RowVertex vertex, Iterator<M> messages);

    /**
    
     * Returns the output type for the function.
     
     */
     
    StructType getOutputType();
    
}


init

算法的初始化接口，主要完成算法的一些初始化操作. PageRank的init方法实现如下：

  @Override
  
    public void init(AlgorithmRuntimeContext context, Object[] parameters) {
    
        this.context = context;
	
        if (parameters.length > 3) {
	
            throw new IllegalArgumentException(
	    
                "Only support zero or more arguments, false arguments "
		
                    + "usage: func([alpha, [convergence, [max_iteration]]])");
		    
        }
	
        // 修正系数，即前面介绍的参数d.
	
        if (parameters.length > 0) {
	
            alpha = Double.parseDouble(String.valueOf(parameters[0]));
	    
        }
	
        // PR值更新阀值，当点的pr差值小于该值时，不再更新pr值.
	
        if (parameters.length > 1) {
	
            convergence = Double.parseDouble(String.valueOf(parameters[1]));
	    
        }
	
        // 迭代次数
	
        if (parameters.length > 2) {
	
            iteration = Integer.parseInt(String.valueOf(parameters[2]));
	    
        }
	
    }
    
process

算法的主要处理逻辑，入参为当前Active点和要处理的消息，PageRank主要实现如下：

    @Override
    
    public void process(RowVertex vertex, Iterator messages) {
    
       ....
    
        if (context.getCurrentIterationId() == 1L) {
	
            // 首轮迭代设置pr初始值,并发送给出边,同时更新当前点的pr值。
	    
            double initValue = 1.0;
	    
            sendMessageToNeighbors(outEdges, initValue / outEdges.size());
	    
            ...
	    
            context.updateVertexValue(ObjectRow.create(initValue));
	    
        } else if (context.getCurrentIterationId() < iteration) {
	
            double sum = 0.0;
	    
            while (messages.hasNext()) {
	    
                double input = (double) messages.next();
		
                input = input > 0 ? input : 0.0;
		
                sum += input;
		
            }
	    
            // 计算当前迭代的pr值
	    
            double pr = (1 - alpha) + (sum * alpha);
	    
            double currentPr = (double) vertex.getValue().getField(0, DoubleType.INSTANCE);
	    
            if (Math.abs(currentPr - pr) > convergence) {
	    
                // pr值发送给出边目标点.
		
                sendMessageToNeighbors(outEdges, pr / outEdges.size());
		
            } 
	    
            context.updateVertexValue(ObjectRow.create(pr));
	    
        } else { // 到达最大迭代次数，结束本轮迭代，take最终计算结果.
	
            double currentPr = (double) vertex.getValue().getField(0, DoubleType.INSTANCE);
	    
            context.take(ObjectRow.create(vertex.getId(), currentPr));
	    
            return;
	    
        }
	
    }
    
getOutputType

定义算法返回类型，PageRank实现如下：

 @Override
 
    public StructType getOutputType() {
    
        return new StructType(
	
            // id
	    
            new TableField("id", LongType.INSTANCE, false),
	    
            // pr值
	    
            new TableField("pr", DoubleType.INSTANCE, false)
	    
        );
	
    }
    
算法注册

算法实现通过注解来定义算法名称，如下所示：

@Description(name = "page_rank", description = "built-in udga for PageRank")

public class PageRank implements AlgorithmUserFunction {

}

算法和UDF一样，需要注册或者创建后才能使用. DSL内置算法或者UDF在BuildInSqlFunctionTable中进行注册.对于非内置算法，可以通过create function语句来创建。

Create funciton page_rank as 'com.antgroup.geaflow.dsl.udf.graph.PageRank';

-总结-

本文主要介绍实时图计算引擎GeaFlow的基本架构，然后介绍了图算法PageRank的基本原理以及在GeaFlow中的实现细节和使用方式。

技术解读 I TuGraph查询引擎简析

存储和计算是图数据库的两个重要组成部分。TuGraph的计算层包括图分析引擎、图查询引擎和图学习引擎。我们之前已经介绍了图分析引擎，现在我们将分享一下图查询引擎。

图查询引擎是一种用于查询图数据库的工具，它使用特定的查询语言（如Cypher或ISO GQL）来执行和操作图数据库。这些查询语言允许用户描述图模式并查询图数据，例如查找特定节点或查找节点之间的路径。

目前，TuGraph支持使用Open Cypher作为查询语言，即将支持ISO GQL查询语言。与关系型数据库的SQL类似，Open Cypher和ISO GQL都是声明式查询语言。它们需要转换成高效的命令式执行计划，以便计算机系统能够理解和执行。查询引擎需要完成理解用户查询意图和转换执行计划的功能。

在本文中，我们将概述Open Cypher/ISO GQL查询如何执行，但不会详细探讨其中的技术细节，以帮助用户从整体上理解TuGraph查询引擎的工作原理。

01  TuGraph查询引擎架构

TuGraph在不断演进，将图查询和图分析引擎融合在一起，旨在提供混合查询和分析能力，并保持一致的使用体验。其架构如下图所示：

02  查询引擎工作流程

TuGraph查询引擎的主要步骤包括：1. 将输入的查询字符串转换为抽象语法树（AST）。2. 优化并归一化AST，生成通用语法中间表示（GST）。3. 使用GST创建查询图并提取符号表，创建逻辑执行计划。4. 优化逻辑执行计划。5. 根据逻辑执行计划创建（物理）执行计划。6. 校验schema约束、访问权限等。7. 执行上述执行计划，得到查询结果。

将输入的查询字符串转换为抽象语法树（AST）

将用户输入的（Open Cypher或者ISO GQL）查询字符串转换为抽象语法树（AST）。在此过程中，任何语法错误都会被捕获并返回给用户，但我们在此阶段不进行语义检查。

优化并归一化AST，生成通用语法中间表示（GST）

在将查询字符串转换为抽象语法树（AST）后，我们仍需要解决一些问题。首先，原始AST层次较高，分支较为繁杂。更重要的是，AST与查询语言相关，不同的查询语言（如Open Cypher和ISO GQL）会生成完全不同的AST，这不利于后续的统一处理。

为了解决这些问题，我们将在此步骤中完成以下任务：

进行语义检查。

根据语义生成通用语法表示（GST），对AST进行压缩，以便按相同语义进行处理。

无论使用哪种查询语言，生成的GST都是相同的。

使用GST创建查询图并提取符号表，创建逻辑执行计划

我们使用通用语法中间表示（GST）来创建查询图，它是查询的更抽象、更高级的表示。使用查询图而不是直接在GST上操作，可以更有效地计算成本和执行优化。

逻辑计划是一棵多叉树，由类似于关系数据库逻辑计划所使用的运算符组成。为每个查询图生成一个逻辑计划（根据查询，查询图可能由子查询图组成）。这是按照自下而上的方法逐步完成的。优化逻辑执行计划在传统的关系型数据库中，关系代数（以及等价的元组关系演算和域关系演算）构成了数据库查询理论的基础。它们也是执行计划优化的理论基础。

TuGraph总结了图代数是SQL关系代数的扩展，如下图所示：

因此，TuGraph的优化器包含了许多优化，例如filter安置、dedup安置、traversal规划、lazy materialization等等。在后续的文章中，我们将详细介绍这些优化。

由逻辑执行计划创建（物理）执行计划

我们为逻辑运算符选择物理实现并缓存执行计划，从而根据逻辑计划创建执行计划。物理执行计划是一棵由运算符组成的树，它与逻辑执行计划同构。其中的每个物理运算符都准确地完成逻辑运算符规定的操作，接收逻辑运算符规定的输入输出。

校验schema约束、访问权限等

在此步骤中，我们会对执行计划的有效性进行校验，包括schema约束、访问权限等。如果校验不通过，我们将报错并返回给用户。

执行上述执行计划得到查询结果

通过校验的执行计划将被实际执行，执行的结果将作为查询结果返回给用户。

03 小结

一款卓越的图查询引擎通常具备如下特质：功能强大、快速执行查询、易于使用、并且能够扩展到更大型的图数据库以提供卓越的性能。TuGraph查询引擎的设计恰是基于这些特质而进行的。我们将在未来用更多文章详细介绍TuGraph查询引擎的设计和每一步的工作。

蚂蚁高性能图数据库TuGraph-DB的技术思考与实践

在近日举行的 DTCC 2022 第十三届中国数据库技术大会-图数据技术与应用创新专场，蚂蚁集团图数据库负责人洪春涛博士分享了蚂蚁高性能图数据库TuGraph-DB的技术思考和实践，以下为演讲内容要点回顾。

图计算的优势

图计算是一种高效的抽象计算方法，可以方便地处理复杂的多维数据。我们将员工和公司之间的关系画成图，这样可以快速查询员工的信息，例如员工的工作情况、与其他员工的关系以及与哪些公司有联系。相比之下，在关系数据库中，我们需要分别建立员工信息表、公司信息表以及员工和公司之间的关系表。这就把整个数据切成了很多张二维表，在实际应用系统中经常能够看到几百上千张表。这样会使得系统变得更加复杂，需要基于多张表去做推断，对人或机器来说都会带来挑战。

对比来看什么是简单查询和复杂查询：

图中表格的前两行属于比较简单的查询，比如某个员工的工龄，直接找到对应的那一行然后取出来就可以；比如找出所有的雇员数大于5的公司，可能在雇佣关系表中就能找出来，在关系数据库里这些都属于常规的操作。

但如果查询再复杂一点，我想知道员工A和员工C之间有什么关系，不一定是一跳的关系，可能包含在同一家公司工作，也可能包含参与了同一个项目，甚至还包含他们有一个共同好友，这些关系就是多种多样的，想用SQL列举所有可能的关系其实就没有那么容易了。如果确定知道这里可能有几种关系，还能靠SQL穷举出来，但随着表越来越多，里面的关系可能有几百上千的时候，再想去穷举就非常难了。

最后还有一种更复杂的查询，比如想找员工A和员工E的所有关系，可能包括A认识B，B认识C，C认识E，相当于在做一个不定长跳数的查询，在SQL里面就非常难写了，相信绝大多数人都难以写出这种查询。

业界有句话，所谓的关系数据库实际上并不擅长处理关系。例如，员工A和E之间的所有关系实际上就是我们要查询的关系，但在关系数据库中处理这种关系查询实际上是不够友好的。这是图数据库的优势所在，它们更擅长处理复杂的关系数据。我们发现，大部分浅显的数据信息都可以比较容易地挖掘出来，但要想更深入地利用数据产生更多的价值，就必须挖掘更深层次的信息，这时就一定会遇到这些复杂的关系，这也是为什么图数据库越来越受欢迎的原因。

为什么图数据库开始流行

图数据库的概念最早出现在七八十年代初期，但当时为什么人们没有选择使用图数据库，而是选择了关系数据库呢？我认为主要原因是，当时的计算机并不那么强大，使用二维表格形式的关系数据库对计算机来说更友好。我们知道，计算机最擅长的事情就是重复的劳动，重复的任务。如果我们要在一张二维表中找一行数据，我们可以一行一行查找，或者使用二分查找（如果数据是树状有序的）。每一层都是重复的算法在运行，这对计算机来说是一个非常规整的数据，容易处理。但如果把数据抽象成一张图，那么难度就会大很多，对计算机来说会更难处理。

举个例子，一个员工可能与其他人有各种不同的关系，比如朋友关系、雇佣关系、参与项目关系，每种关系的类型都不同，对应的数据也都不同。此外，一个点上的边数也非常不规律，有的人可能只有几个好友，而在微博上，一个普通人可能有两三百个粉丝，而大V账号可能有数百万甚至上亿的粉丝。这样一来，存储数据时，普通人和大V之间的差距就非常大了，对计算机来说，处理这两种数据的差异也会很大。

我们知道，在七八十年代，计算机相对来说很弱，如果那个时候使用图来表示数据，整个处理和查询的难度就会大很多。所以，人们选择使用关系数据表的形式表示数据。事实上，那个时候有人做过图数据库，但并没有流行起来，原因就是性能相对关系数据库来说差得太多了。

我们知道，所有的事物，尤其是电脑，一直在不断进步。这包括硬件和软件方面的改进。如今的硬件和几十年前的硬件完全不是一个概念，而现在的软件优化也大不相同。随着这些改进，我们会发现对当前的电脑和计算机系统来说，使用图数据库带来的额外开销可能不是很大的问题。

举个例子，我们会发现，过去人们都使用低级编程语言编写程序，但随着时间的推移，有了高级语言。比如最开始、最原始的电脑可能是用纸袋和机器码写程序，后来有了汇编，再后来有C语言、C++，现在很多人都直接写Python。虽然 Python 程序的执行速度可能较慢，但是写的很快，而用用C++或者汇编去写得写半天，对于编写程序到最终得出结果的整个过程来说，使用 Python 会更方便。

计算机编程语言的发展是从低级向高级演变的，数据抽象也是一样。我们认为，未来的数据抽象一定会对人更友好一些，而不是专注于对机器更友好。如图所示，编程语言的发展是从低级语言向高级语言转变的，我们也认为数据抽象层次也会慢慢从低层次表格抽象向高层次图表抽象发展。

图计算在蚂蚁的应用

自2015年开始，蚂蚁实际上投入了大量资源来研究图计算，研究如何在蚂蚁的业务中使用图计算。例如数据血缘应用。在对业务的处理过程中，我们需要较好地追踪这些数据的流转路径，如果修改了一份源数据会对下游数据产生什么影响，会对最终业务产生什么影响。为了更好地追踪，我们使用图数据库来存储数据。

另一个比较有趣的应用场景就是程序分析。相信几乎所有互联网公司内部都有大量的程序，因此，我们需要管理这些程序，并在每次提交代码时了解将会对哪些内容产生影响。为此，蚂蚁负责程序分析的团队会分析这里的图数据。例如，定义一个变量A，然后使用变量A，“定义”与“使用”之间就会有一条边，使用关系会存储在图数据库中。目前我们的图中已经有超过200亿条边，这是一个非常大的数据量。我们需要对这些数据进行存储、查询和分析，这是蚂蚁公司内部非常多的图数据场景之一。

举个例子来说明优惠券反套现的场景：满额返券是一种比较常见的促销方式，比如购物满2000元就可以享受100元的优惠。这种情况下，如果正常消费，用户花费2000元，通过返券省下100元。但是有些人会想办法注册假商铺，进行虚假交易，目的是把平台补贴的优惠券套出来。因此当用户去买东西，平台要去付补贴的过程，我们需要去实时检测一下会不会有可疑的资金交易情况。

蚂蚁有很多业务需要研究图计算系统和图数据库等技术来满足需求，因为这些业务需要对大量的点边进行分析，数据量超过了100TB，基本上已经达到了PB级别。我们需要对这些图进行实时查询，吞吐率大约在百万级别。由于需要对用户的付款进行实时判断，所以需要比较低的延迟，大约在20毫秒的级别。如果延迟太长，会导致用户体验很差，比如付款需要等待5秒才能完成，这样就会非常麻烦。

图计算系统建设中的问题与挑战

在建立蚂蚁图计算系统的过程中，我们遇到了各种各样的问题。为了解决这些问题，我们与学术界和许多研究界的同事一起合作，并发表了许多相关的学术论文，包括EuroSys等。然而，我们在建立系统的过程中发现，目前的图计算仍处于较早期的阶段，因此许多标准尚未成形。这对我们来说是一个棘手的问题。例如，在关系型数据库中，查询语言基本上就是SQL，但在图数据库中，仅查询语言就有许多种，包括Gremlin、G-SQL等等。这导致了市场的碎片化，人们学习和使用的成本也很高。

在建立图计算系统的过程中，我们也遇到了许多挑战。为了分担较大的通信量，需要将图数据分布到多台机器上，但这会导致边的信息在不同机器之间传递，造成大量的通信。此外，单次查询所涉及的数据量也比较大，例如五跳查询涉及的点数就已达到10的五次方，图中还存在一些非常大的点。同时，用户对图计算系统的需求也十分多样，既有快速查询的需求，也有对复杂算法（如社区发现）的需求，单一系统很难满足这些不同的需求。

TuGraph技术优势

蚂蚁自己开发了一套图计算系统TuGraph，既能解决图数据的存储问题，也能解决流式计算、离线计算和图学习的问题。目前，超过100个业务线和300多个场景都在使用这套系统。这套系统在2021年获得了世界互联网大会领先科技成果奖。

在TuGraph中，性能是一个重要的因素，因为图数据集的体积很大，如果性能不佳就会浪费机器资源，导致许多情况下无法完成任务。比如，希望业务的查询能在几十毫秒内返回结果，但是如果做的性能不好，几秒钟才能返回结果，就无法作为在线查询使用。因此，我们是非常对性能是很重视的，其中在 LDBC-SNB 标准测试中（类似于数据库领域性能标准测试TPC-C），TuGraph仍然是世界纪录的保持者。

TuGraph 的整个图存储是建立在完美哈希的基础上的，这是我们与其他图系统的一个重要区别。目前，大多数图系统使用的是基于数的存储，但数的问题在于永远存在一个 LogN 的查找操作。然而，在图中可以看到，不同的顶点之间实际上是无序的，不需要有顺序，所以顶点这个级别实际上是基于哈希的，理论上，顶点的读取是最优的。

此外，TuGraph 还参与了许多标准的定制，整个系统在尽量往标准化的方向去做。

除了为内部提供服务，我们还向外提供服务，主要是因为，作为一个系统，如果只为有限的客户提供服务，就很容易构建成一个专有系统。我们希望这是一个标准化、开放的系统，所以我们也在对外提供图计算系统的产品和服务。目前，我们也有很多外部客户，包括金融、工业、互联网以及政企领域。

开源开放，共建发展

整个图计算系统目前仍处于较早期的阶段，我们认为还有很多工作要做，包括提升应用性、性能和降低成本。所有的系统都会有这些问题。但是，如果希望普及，我们认为最重要的是有健康的生态，来推动图计算系统的发展，需要有更多的用户和更多的场景使用这个系统。

所有的计算机系统都需要去有一个更开放、更大的生态才能促进发展。蚂蚁有一句话叫做“成熟一个、开放一个”，一个系统成熟以后，我们就会试着开放出去，让更多的人去用。今年9月，我们已经在GitHub上开源了TuGraph中的单机版图数据库，以及一个离线图分析引擎TuGraph Compute。分布式图数据库和流式图计算现在已经包含在我们的商业化版本中，包括一站式图研发平台。我们计划在未来迭代更多更丰富的系统功能，希望能做得更好。

TuGraph开源版特色

为什么要去开源单机版而不是分布式版本？主要是考虑到它的部署和使用成本比分布式版本要低得多，同时功能也很完整、独立。我们希望这样可以让许多刚开始使用图数据库或有使用图数据库解决问题的想法的人，可以先尝试用我们的单机版图数据库。因为它的部署非常简单，如果跑起来没有问题，那么再考虑是否需要分布式版本。如果确实需要，我们可以再跟进这个问题。

我们的单机版图数据库已经能够支持TB级别的数据，我们内部也有很多情况使用单机版图数据库。在单台机器上，我们最大的数据量也达到了2TB多，在线上运行，能够处理百亿级别的点边。事实上，大多数用户使用单机版图数据库都是足够的。由于单机版的图数据库很容易优化，我们对它进行了极致的优化，因此单机版图数据库在性能上可以满足绝大多数场景的需求。此外，它的系统特性也很全面，包括高可用性、多图支持、权限管理、日志记录等，它可以被看作是一个成熟、易用的图数据库，类似于MySQL。

图中所列出的开源版TuGraph几个特性包括：

单机版图数据库能够处理数据量几个TB的数据，前提是磁盘足够大。

成本很低，因为是单机版，部署和运维都很容易。

性能很好，我们对其进行了大量优化。TuGraph的LDBC-SNB测试目前是世界第一，大家可以在GitHub上获取测试SNB的条款并进行测试。

单机版图数据库是一个非常易用的完整系统，我们提供了导入导出工具和查询语言。此外，还提供了底层API，用户可以使用它来编写复杂的程序。

我们的开源版本的目标主要有三点：

首先，我们希望提供一个免费的图数据库产品，能够让更多的人使用图数据库，尝试用它来解决问题。

其次，我们希望促进图数据库标准的成形。目前图数据库的差异太大，每个数据库都有所不同，我们希望通过提供一个参考答案来帮助大家达成趋同。这样大家就可以根据我们提供的设计来判断哪些特征合理，如果觉得合理就可以遵循这个设计，慢慢地大家就会逐渐靠近。假如所有产品在主要特征上保持一致，这样所有人的学习成本就会降低。

最后，基础研究性问题可以不断优化发展，包括存储方面的问题，例如哈希可能是理论上最优的，但是是否还有其他需要调整的东西？目前没有一个很好的研究性平台让大家去进行这些尝试和研究，我们希望提供的开源TuGraph-DB能成为这些研究人员的对比基线，促进研究的发展。

TuGraph企业版特色

除了开源版本，我们也继续提供商业版本。这个版本包含一个分布式图数据库，以及离线计算引擎和流式图计算功能。此外，我们还提供了TuGraph Platform一站式图平台，包括运维、可视化等功能。在这个平台上，用户可以在图数据库中执行流式计算，并在线写回数据库。这种方式通常用于实时查询结果，因为流式计算的时间可能比较长，但用户可以立即查询到较早的结果。这对于在线业务来说非常重要。

商业化产品还提供私有化部署，也可以通过一体机的方式部署硬件，并将很快推出云上部署方案，这样大家就可以在云上体验我们的产品。

总结

蚂蚁在图计算方面投入了大量资源，并在众多业务场景中磨练出了一整套在线查询、流式计算、离线分析以及图学习的体系。目前，我们已经在GitHub上开源了单机版（https://github.com/TuGraph-db），同时也提供企业版来满足不同用户需求。

演讲回顾｜TuGraph-DB兼容Neo4j客户端：Bolt 协议设计与实现

今天要和大家分享的是关于TuGraph-DB近期新添加的一个功能——兼容Neo4j客户端。这个兼容性工作本质上是实现了Neo4j客户端与服务器之间的通讯协议，即Bolt协议。首先介绍下Bolt协议的基本情况，之后阐述下我们是如何实现这一协议的，然后展示下使用Bolt协议客户端连接TuGraph-DB的效果，最后会说明下目前还有哪些功能尚未实现。

背景

Bolt协议是在Neo4j 3.0版本中引入的，时间大约是2016年。在此之前，Neo4j使用的通讯方式主要是HTTP和WebSocket。自从引入Bolt协议后，已经过去了7到8年，目前它已经成为了Neo4j应用与服务器之间通信的首选方式。

Bolt协议在设计上具有以下几个特点：首先，它是二进制的，与之对应的HTTP协议是基于文本的；其次，Bolt协议支持事务、会话以及集群模式，并且提供了用户身份验证和数据加密功能；最后，Bolt协议中所有的数据传输都是基于流式传输的。

TuGraph-DB的客户端也经历了不断的演进和发展。最初我们仅支持HTTP方式，后来开发了自己的RPC客户端，现在我们开始支持兼容Neo4j的客户端。

兼容Neo4j客户端的最大好处在于生态系统的兼容。单就客户端而言，Neo4j官方开发了五种语言的客户端，加上社区开发的两种，总共有七种语言的客户端可供使用。这些客户端可以直接拿来使用，同时还有一些对接上下游生态系统的组件，比如访问Apache Spark或Apache Kafka，这些都有现成的代码。此外，还有一些开源项目，比如DataX，对Neo4j的适配的代码也是现成的。另外，还有一些编程框架相关的，特别是在Java领域，例如OGM、Spring等开发框架，这些相关的代码也无需重写，可以直接使用。这样就节约了大量的研发资源，周围组件直接用生态内的，我们可以将更多精力投入到数据库自身能力的提升上去。

Bolt协议介绍

Bolt协议主要由三大部分构成。首先是数据结构的编码和解码，它作为一种通信协议，核心功能就是数据传输。传输过程中，数据首先需要被编码，然后在网络上进行传输，接收端再对数据进行解码。其次是消息类型，这个比较常规，每个通讯协议都会定义自己的消息类型集合，主要用于控制和管理通讯过程。最后是消息处理机制，即以状态机的运作方式来处理Bolt协议中的消息，它不是简单的Ping-Pong模式。

数据编解码

在Bolt协议的数据编解码部分，有一个关键词是PackStream。它是Bolt协议中的一种数据序列化方式，源自于MessagePack。MessagePack也是一种数据序列化方案，类似于我们常用的Protobuf。

MessagePack在Redis里面有被使用到，大家都知道Redis是一种内存数据库，它对性能的要求非常高，所以MessagePack有一个特点是序列化和反序列化速度很快。但Bolt协议并没有直接使用MessagePack，而是根据其需求在MessagePack基础上定制了一套名为PackStream的序列化方式。在PackStream序列化方式中，包含几种核心的数据类型，如常用的布尔、整型、浮点型、二进制数组，和类似于字典（dictionary或map structure）的结构体。

由于Bolt协议主要用于传输Cypher查询语句及其响应结果的，所以Cypher的数据类型必然是构建在上述基础数据类型之上的。例如，Cypher数据类型中的节点（Node）、关系（Relationship）和路径（Path），都属于PackStream中的结构体，空间类型和时间类型也是。

左下角的小图解释了PackStream的序列化方式：数据序列化完成后，它的大致结构是。第一个字节表示数据类型，用以标记当前数据是哪种类型。紧接着是size字段，表示数据大小，然后是数据内容本身。这只是一个通用的表示方法，大部分数据序列化后形态都类似，但许多场景下是可以进行优化的。例如，某些数据类型在编码后仅为一个字节，因为它与前面的标识字节合并在一起了。对于一些数值小的整数，如只需一个字节就可以表示的数字，虽然数据类型本身占8个字节，但最终编码完成同样只有一个字节大小，它们的类型和数据本身均被压缩在一个字节中。

消息类型

Bolt协议中的消息主要分为三大类。第一类是Request消息，这类消息是由客户端发送给服务端的；第二类是Summary消息，这是服务端发给客户端的消息；第三类是Detail消息，目前只有一个Record消息，Record消息就是服务端返回给客户端的一行行数据，接下来简要介绍下每种消息。

Hello消息是TCP连接建立后需要发送的第一条消息，它会携带客户端自身的信息，比如客户端是什么语言的，版本号是多少，以及运行平台是什么，还有进行身份验证所需的用户名和密码。Reset消息用于重置会话状态，因为Bolt协议是有会话的，会话就有会话状态，有时候会话会处于失败状态，这时候就需要用Reset消息来复位一下。

Run消息用于执行Cypher查询语句，它携带了查询语句的具体内容。Pull消息是客户端请求服务端拉取数据，一般后面会跟个数字，例如Pull 1000，代表客户端希望拉取1000条数据。服务端接收到这个请求后，会返回不多于1000条数据。如果服务端在返回数据后发现还有更多数据可供返回，它会发送个Success消息，Success消息里面的has_more字段会置1，提示客户端还有更多的数据待发送。客户端收到后会继续发送Pull消息，如此循环，直到服务端返回的Success消息中has_more字段不为1，表示所有数据都拉取完了。Discard消息用于告诉服务端忽略后续特定数量的数据。

Pull和Discard消息可以交替使用，例如客户端可以轮流提取和忽略1000条数据。Begin、Commit和Rollback消息用于显示事务，而Route消息则用于获取集群的路由信息。Goodbye消息用于断开当前的连接。Ignore消息在服务端无法处理任何请求时使用，这时候服务端将回复一个Ignore消息，表示上一个请求没处理，直接丢掉了。Failed消息表示标准的失败响应，而Record消息则用于传输真实数据，例如传输1000行数据时，就会连续发送1000个Record消息给客户端。

服务端状态

Bolt协议消息是按照状态机的方式进行处理的，状态机就涉及到有哪些状态以及这些状态是怎么转换的。右侧的图表是从官方协议文档中摘录的，描述了状态转换的过程。由于图表较长，只截取了一部分，这里仅简要介绍几个最常用的状态。

首先是Connect状态，这是当TCP连接刚建立后的初始状态。随后，客户端发送Hello消息，一旦服务端处理成功，会话进入Ready状态。如果客户端发送Run消息，即执行Cypher语句，服务端随后会进入Streaming状态，此时会话涉及到数据传输。如果服务端的数据尚未发送完毕，它将持续保持在Streaming状态，直至数据完全发送完成后回到Ready状态。

Tx_Ready和Tx_Streaming状态是处理显式事务时使用的。其中，Tx Ready是在客户端发送Begin请求开启显式事务后达到的状态；Tx Streaming与上面的Streaming类似，只不过它发生在显式事务中。Failed状态则意味着当前会话处于失败状态，这种情况很容易发生，例如客户端不小心发送了一个有语法错误的Cypher语句，服务端报错后，会话将进入Failed状态。根据协议的状态机规则，在Failed状态下，服务端无法处理任何消息，如果客户端继续发送语句，服务端会直接忽略并回复Ignore消息。为了继续使用当前会话，客户端需要发送Reset消息，以重置失败状态并让会话回到Ready状态，继续提供服务。


交互举例

以执行一条Cypher语句为例，下面列出了相关的Bolt消息。首先，TCP连接建立后，客户端会发送固定四个字节的数据，这是Bolt协议的标识。紧随其后是发送16个字节的版本号数据，总共4个版本，这是因为Bolt协议存在多个版本。客户端发送支持的版本号给服务端，服务端选择一个共同支持的版本后回复给客户端，这个过程称为版本协商。成功后，客户端和服务端间的通信将在选定的协议版本下进行。

接着客户端发送Hello消息，包含其身份信息以及用户名和密码。服务端验证成功后返回Success消息，并且会提供服务器的信息以及分配的连接ID。然后客户端发送Begin消息来请求开启一个显式事务，其中包括多个参数，如事务模式和数据库名称等。服务端确认后发送Success消息，客户端随后开始发送Run消息执行Cypher查询，并接收由服务端返回的结果。

这里，客户端请求只读取两条数据，那么服务端接着连续发送两个Record消息。随后服务端发送附带has_more字段的Success消息提醒客户端还有未发送的数据，但是客户端选择发送Discard消息来告诉服务端忽略余下数据并继续处理。

最后，客户端提交事务，如果成功，服务端会返回Success消息。

另一个例子说明了Bolt协议的另一个特性，即pipeline流水式处理。客户端可以连续发送多条Cypher查询语句，而不需要等待前一条语句执行完成。例如，客户端连续发送两条“Run”消息，然后使用Pull消息来指定拉取某个特定查询语句的结果，这需要在消息中明确指出语句的ID，以便服务端知道客户端希望获取哪一条查询的结果，但这也要求服务端也必须具备相应的能力，即在一个会话中可以同时执行多条语句。

Bolt协议实现

接下来，我将简要描述TuGraph-DB是如何实现Bolt协议的。起初我们当然会进行搜索，看是否已经有现成的开源实现可以利用。是有一个C语言版本的，但这个C语言实现版本相当陈旧，已经很多年未更新，这个是社区贡献的，并且只支持到Bolt协议的3.0版本，而目前Bolt协议已经发展到了5.0版本，3.0版本可能很快就会被淘汰。因此，我们没有采用这个版本，Neo4j自己是没有没有开发C++版本的客户端的。

后来我们研究了官方的Bolt Golang语言的驱动代码。经过阅读后，我们发现Golang驱动里面Bolt协议这块代码写得比较清晰，于是决定以此为参考，使用C++语言来实现Bolt协议。

在网络框架方面，因为Neo4j是使用Java编写的，Neo4j自己在实现Bolt Server的时候用的是Java里面的Netty框架，Netty是一个NIO网络编程框架，在C++中则可以使用有类似功能的Boost.Asio网络框架来实现。至于消息处理部分，我们严格按照官方文档中描述的状态机规则来构建。

在线程模型的设计上，核心理念是每个连接对应一个线程。主要是因为会话是有状态的，将每个会话绑定在一个固定的线程上，处理起来比较方便。类似地，MySQL也是采用这种方式，为每个连接直接启动一个线程来处理。

关于协议的版本，我们目前采用的是Bolt的4.4版本。Bolt协议有多个主要版本，包括1.0、2.0、3.0、4.0和5.0，每个大版本下还有多个子版本号。而4.4版本是官方的长期支持版本，支持到2026年。


数据编解码

数据编解码这块的实现，主要方法是参考官方Bolt Golang驱动的代码，然后用C++进行翻译。这里举了个例子：Bolt协议里面int64数字的编码实现，左边是Go版本的，右边是C++版本的，可以看到几乎是逐行把Go的语法转换成了C++的语法。

数据编解码它是纯粹的计算逻辑，也可以选择参考官方的协议文档自己编写，但这样效率相对较低，而且枯燥，并且出错的概率也大，对着已有的版本翻译是最快的。但是相应的测试代码也得翻译过来，并且跑通，确保逻辑正确。

Bolt Server 网络模型

这张图是Bolt sever的网络模型，最左边是监听线程，中间是一堆的Bolt IO线程，每个线程处理一些Bolt连接的IO读写，每个连接固定对应右边的一个状态机线程。当消息从Bolt连接里面读出来后，会放到状态机线程的消息队列里面去，然后发个信号唤醒状态机去处理消息。消息的处理是按顺序的，如果没有消息。状态机线程处于休眠状态。

不兼容的点

我们也注意到了一些与官方Neo4j客户端的兼容性问题。Bolt协议是Neo4j自己设计的，服务与自家的产品，并不是一个通用的标准，所以Bolt协议里面不可避免的会有一些Neo4j独有的特性。比如在新版本Neo4j中点的结构体中有一个element_id的字段，是字符串类型类型，这个在TuGraph-DB中没有这个东西，如用Neo4j客户端连TuGraph-DB，在客户端代码中去获取这个字段，目前返回是空的，而上面的三个字段是有的。同理，右边RUN消息里面红框标出来的都是目前TuGraph-DB没有的。

使用效果

我们编写了一段简单的Java代码来展示使用Bolt客户端连TuGraph-DB的效果，与关系型数据库里面的JDBC非常类似，整体的使用体验很好。此外，我们还开发了一个Console 客户端，通过Bolt协议与服务端通信，也可以用这个CLI客户端实现在线流式导出数据。在性能对比方面，我们发现新实现的Bolt客户端在性能上略优于我们之前自己编写的RPC客户端，原因可能有多方面，例如编码方式的不同或是PackStream的序列化速度更快。暂未实现的功能目前尚未实现的功能包括显式事务、集群模式、加密传输和高级身份认证。尽管官方Bolt协议支持这些功能，但由于支持它们需要一些改造，且成本与收益比并不高，因此我们当前只支持基本的事务和身份认证。不过，我们计划在今年适配集群模式。

蚂蚁开源图数据库 TuGraph-DB升级到 v4.0，全新支持 GQL 国际标准查询语言

在2022年9月的世界人工智能大会上，蚂蚁自研图数据库TuGraph-DB 正式开源。正值开源一周年之际，迎来了开源TuGraph-DB v4.0版，期间经历了10次重要迭代，重大更新的功能包括云上部署、POG（Procedure On Graph query language）、Python算法接口、图学习引擎、高可用、ISO GQL等，在功能完备性、上手易用性、设计先进性等方面做了大量的工作。

TuGraph-DB v4.0主要支持了ISO GQL国际标准查询语言、企业级高可用能力以及图学习引擎。

TuGraph-DB v4.0 核心新功能介绍

1.ISO GQL

关系型图数据库之所以能够顺利发展，一个重要的原因是标准化查询语言SQL的广泛使用，既降低了数据库用户的操作门槛，同时抹平了不同数据库间的实现细节，形成了分工合作的生态。

GQL（Graph Query Language）是一种用于图查询语言的国际标准，是图数据库领域的“SQL”，参与方包括国际标准机构、高校、图数据库厂商、非盈利组织等，是图生态中非常重要的一环。

新版本遵循GQL的最新标准，在查询引擎进行了支持，为用户提供了丰富多样的查询语言选择，也对图数据库领域查询语言的标准化起到了推动作用。

2.企业级高可用

TuGraph-DB开源了企业级高可用能力，能够实现多活热备。支持用户便捷使用客户端来操作高可用功能，自动负载均衡，数据库不需要额外配置就可以支持更高的读负载。
在TuGraph-DB高可用集群模式中，服务器中有一个leader和多个follower，可以并发提供读写能力。在任一单个节点故障时，基于RAFT协议可以秒级切换leader，持续为业务提供读写服务，RPO=0，RTO<10秒。

TuGraph-DB 高可用后续还将支持Witness、Learner 等更多的功能角色。


3.图学习引擎

TuGraph-DB v4.0 深度集成了图学习引擎，兼容DGL、PyG等常见图学习框架。TuGraph-DB提供两种典型的图采样算子，包括对最新数据进行实时图采样，和基于快照进行批量图采用。学习引擎的实时图采样和查询引擎的图查询使用同一份存储数据。TuGraph-DB用户能够在图数据库上直接进行图学习训练。

用图数据库作为图学习引擎的后端，最显著的一个优势是可以支持充分利用硬盘空间，即图学习涉及的数据不用全部放到内存。相比较传统的大数据引擎的解决方案（比如Spark），TuGraph-DB的图学习引擎可以在单机进行超大规模的图学习，大大降低了部署成本。


结语：

在过去的一年里，不论是接口的丰富性还是学习引擎的支持，TuGraph-DB都致力于在简单易用性和功能完备性方面取得更好的表现。在未来的规划中，TuGraph-DB将持续进行架构演进，努力打造更活跃的图生态。


源码解读｜TuGraph key-val存储

存储模型

TuGraph存储层采用开源的lmdb实现，底层存储采用了B+树，在B+树之上抽象出了一层key-val存储。它与基于hash的key-val存储有着本质的区别，基于hash的key-val存储在无hash碰撞的情况下，一次key-val的查询只需要一次hash计算和一次内存寻址，时间复杂度是O(1)，但在TuGraph的实现中，key-val层是基于B+树来实现的，key-val查找的本质还是基于B+树的范围查找，因此一次key-val的查询是要在树中进行二分查找的，性能是O(log(n))。 

TuGraph基于lmdb的存储模型，对由点边数据组成的key-val进行了包装，将点边数据打包保存在一起，对于非大点，通过一次key-val的查询就可以查找到点以及该点的出边和入边，减少了在B+树中进行二分查找的次数，将基于全图完整数据的查找限定在某个key对应的value中，提升了TuGraph在适用场景下的查找性能。对于大点来说，为了避免某个key对应的value过大，影响写入或修改性能，权衡之下将点和边数据以阀值按类型切分为多个value，分别保存在不同的类型的key中，既提升了查找性能，也降低了大点带来的写入性能损失。

key的类型

enum PackType {

    PACKED_DATA = 0, 
    
    VERTEX_ONLY = 1, 
    
    OUT_EDGE = 2,    
    
    IN_EDGE = 3     
    
};

PACKED_DATA

PACKED_DATA类型的key共计6字节，由5字节的点id和1字节的类型组成，此类key对应的是混合存储的val。将某个点和与之关联的边数据打包在一起保存，适用于非大点的存储，新插入的点也属于此类型。

VERTEX_ONLY

VERTEX_ONLY类型key共6字节，由5字节的点id和1字节的类型组成，此类key对应的val中只保存了单点的属性，当一个PACKED_DATA类型的key对应的value长度超过阀值::lgraph::_detail::NODE_SPLIT_THRESHOLD(1000字节)时，会被拆分成VERTEX_ONLY，OUT_EDGE，IN_EDGE三种类型，分别表示一个单独的点，一组从该点出发的边，和一组目标点为该点的边。

OUT_EDGE

OUT_EDGE类型key共计25字节，由5字节的源点id，1字节的类型，2字节的边label id，8字节的tid，5字节的目标点id，以及4字节的eid组成。此类key对应的val中保存了由src_vertex_id点出发，按label_id和tid以及目标点id排序好的一组出边。适用于大点切分后保存出边。其中key中的源点id和类型对于value中的所有值都是一致的，label id，tid以及目标点id是取了这组边中最后一个边的属性。

IN_EDGE

IN_EDGE类型key共计25字节，由5字节的目标点id，1字节的类型，2字节的边label id，8字节的tid，5字节的最大源点id，以及4字节的eid组成，此类key对应的val中保存了指向dst_vertex_id，按label_id和tid以及源点id排序好的一组出边。适用于大点切分后保存出边。其中key中的目标点id和类型对于value中的所有值都是一致的，label id，tid以及目标点id是取了这组边中最后一个边的属性。

获取key中对应的属性

可以通过key的地址加上对应属性的字节偏移获取对应的值。

static const size_t FID_OFF = 0;                  // first vid  0

static const size_t PT_OFF = FID_OFF + 5          // pair type  5

static const size_t LID_OFF = PT_OFF + 1;         // label      6

static const size_t TID_OFF = LID_OFF + 2;        // primary id 8

static const size_t SID_OFF = TID_OFF + 8;        // second vid 16

static const size_t EID_OFF = SID_OFF + 5;        // edge id    21

static const size_t EDGE_KEY_SIZE = EID_OFF + 4;  //      25

val的类型

VertexValue

VertexValue类型的值用于保存key为VERTEX_ONLY的值，代表单个的点数据，数据格式按照预定义好的schema来保存，存储格式如下：

EdgeValue

EdgeValue类型的值用于保存key为OUT_EDGE与IN_EDGE的值，代表从某个点出发，或者指向某个点的一组边数据。存储格式如下，绿色部分是EdgeValue，它包含三部分内容，第一部分是1字节的边个数，因此一个EdgeValue类型的数据中最多只能保存255条边，第二部分是一个偏移数组，代表了每条边在value中的偏移位置，第三部分是边的内容数组，每个成员代表了一条边。 

边的内容包含edge header 和 edge value两部分。edge header 由一字节的指示器开始，它的作用是标识后面紧跟着的几个字段长度。edge value 保存的是按照 edge schema 序列化好的数据。这里为了节省磁盘空间，对edge_label_id, temporal_id,vertex_id以及edge_id进行了压缩，采用的压缩算法如下： 

edge_label_id：如果edge_label_id为0则不占用存储空间，如果edge_label_id < 0x100(十进制256)则占用1字节，如果 0x100 <= edge_label_id < 0xFF则占用两字节 

temporal_id：如果temporal_id为0则不占用存储空间，不为0则占用8字节存储空间

vertex_id：去除vertex_id中前导的0，按8位对齐 

edge_id：去除edge_id中前导的0，按8位对齐


PackedDataValue

PackedDataValue类型的值用于保存key为PACKED_DATA的value，代表将某个点与其出边，入边一起打包后保存，因为将点和边打包在同一段存储结构中，因此结构复杂，它包含两个int32_t类型的偏移，分别指向这个点的出边和入边的首地址，以及一个VertexValue表示一个点和两个EdgeValue，分别表示这个点的出边和入边。存储格式如下，其中红色部分是PackedDataValue，绿色部分是EdgeValue，紫色部分是VertexValue(保存的是按照vertex schema序列化好的点数据)，蓝色部分是按照edge schema序列化好的边数据。

源码解读｜TuGraph Schema

什么是schema

TuGraph作为强schema的图数据库，在插入数据之前要求数据一定归属于某个schema，即创建某个类型的点边模型之后，才可以插入数据，那么什么是schema呢？schema是一种约束和规定，它有两个方面的作用：

它规定了某个类型的点边拥有哪些属性，这些属性是什么类型的，属性是如何存储在磁盘中的，是否可以为空，它有多少定长的属性，多少变长的属性。

如何从一个点或边数据中解析出某个属性值等等功能。

总而言之，schema指导了如何根据磁盘中的一段二进制数据解析出某个点或边的属性内容和属性值，用于后续的计算。

TuGraph Schema

schema的定义

schema的定义具体表现为一种约束，它指定了一个点或者边有哪些属性，属性大小，以及如何排布。它的定义主要由如下类提供：

schema的应用

schema的应用具体表现为一种格式，它指定了一个点或边的数据(即点或边的属性值)如何存储在磁盘中。下文中所说的点或边的属性值的二进制即指磁盘中的点或边数据。

点的存储格式

点的存储格式一共包含如下几部分内容：

Label Id ：schema的唯一标示，uint16_t类型，因此一个子图(即一个LightningGraph)中的点或边的个数不能超过65535个，超过将会无法区分。

Null-Array ：值可以为空(即optional)类型的属性数组，每个optional的field占一位，以字节对齐，第一个optional的属性标记位于Null-Array第一字节的最后一位，第七个optional的属性标记位于Null-Array第一字节的最前面一位，用于指示某个点或边optional属性对应的数据是否为空。

Fixed-fields ：定长属性列，属性值紧挨着保存，定长属性的长度确定，因此只要知道开始位置就可以正确取出内容，起始位置保存在FieldExtractor类的成员data_off中。

Variable-Offsets ：变长属性偏移列表，记录变长属性开始的位置，变长属性通过当前属性开始位置和下一个属性开始位置的差值计算长度，通过开始位置和长度可以正确取出内容。

Variable-Data ：变长属性内容列表，记录变长属性的值。

边的存储格式

边的存储格式一共包含如下几部分，LabelId作为key的一部分，因此不需要保存在数据中：

Null-Array ：值可以为空(即optional)类型的属性数组，每个optional的field占一位，以字节对齐，第一个optional的属性标记位于Null-Array第一字节的最后一位，第七个optional的属性标记位于Null-Array第一字节的最前面一位，用于指示某个点或边optional属性对应的数据是否为空。

Fixed-fields ：定长属性列，每种属性紧挨着保存，定长属性的长度确定，因此只要知道开始位置就可以正确取出内容，起始位置保存在FieldExtractor类的成员data_off中。

Variable-Offsets ：变长属性偏移列表，记录变长属性开始的位置，变长属性通过当前属性开始位置和下一个属性开始位置的差值计算长度，通过开始位置和长度可以正确取出内容。

Variable-Data ：变长属性内容列表，记录变长属性的值。

schema使用场景

通过schema可以将一段磁盘中的二进制数据解析成某种类型的点或边，可以获取点边属性，也可以将一些值拼接成某个类型的点或边属性的二进制值，存入磁盘中。

获取点索引

通过transaction类获取某个label_name对应点的schema，就可以通过schema获取到具体的索引信息： 

Schema* schema = 

Transaction::curr_schema_->v_schema_manager->GetSchema(label_name)；

for (auto& fid : schema->indexed_fields_) {

    _detail::FieldExtractor* fe = schema->fields_[fid];
    
    VertexIndex* index = fe->vertex_index_;
    
}

获取边索引

通过transaction类获取某个label_name对应边的schema，就可以通过schema获取到具体的索引信息：

Schema* schema = 

Transaction::curr_schema_->e_schema_manager->GetSchema(label_name)；

for (auto& fid : schema->indexed_fields_) {

_detail::FieldExtractor* fe = schema->fields_[fid];

VertexIndex* index = fe->edge_index_;

}

解析LabelId

获取到一段代表点或边属性的二进制之后，通过读取前2字节就可以获取到label id： 

char* p = value.Data();  // 表示一个点或边属性值的二进制形式

LabelId id = *(uint16_t*) value;

解析定长属性

获取到一段代表点或边属性的二进制之后，通过如下方式可以获取到定长属性的值： 

FieldExtractor fe = schema.fields_[schema.name_to_idx_.find(field_name).second];

char* p = value.Data(); 

// 通过起始位置 + 定长属性的偏移位置与属性长度可以获取属性值

Type val = *((Type*)(p + fe.data_off));


解析变长属性

获取到一段代表点或边属性的二进制之后，通过如下方式可以获取到变长属性的值： 

FieldExtractor fe = schema.fields_[schema.name_to_idx_.find(field_name).second];

char* p = value.Data(); 

// 通过起始位置 + fe.v_offs获取Variable-Offsets的位置，记为 pos

// 通过pos + fe.idx * sizeof(int32_t)可以获取到指向该可变属性值的起始位置的指针

Type val = *((Type*)(*(p + fe.v_offs + fe.idx * sizeof(int32_t))));

解析属性是否为optional

对于optional的属性，值可以为空，当值为空时，该属性将不会保存任何值。如何判断该属性值是否为空，需要借助于Null-Array： 

FieldExtractor fe = schema.fields_[schema.name_to_idx_.find(field_name).second];

char* p = value.Data(); 

// 通过起始位置 + fe.nullable_array_off_可以得到Null-Array数组

// 如果对应的位为1，则为空，否则不为空

bool is_nul = (p + fe.nullable_array_off_)[fe.null_bit_off_] 

& (0x1 << (fe.null_bit_off_% 8));

TuGraph Analytics交互式图查询：让图所见即所得

TuGraph Analytics提供了OLAP图分析能力，实现图上的交互式查询，用户在构图并导入数据之后，可以通过输入GQL语句对图查询分析，并以可视化的方式直观地展示点边结果。

OLAP架构

在TuGraph Analytics OLAP架构中，主要以下组件:

Client: 用户通过Client提交查询语句, Client负责和Coordinator交互，发送查询请求。

Coordinator: 接收来自Client查询请求，将查询中的GQL语句进行解析、优化，构建查询的执行计划（执行计划的生成逻辑可参考《分布式图计算如何实现？带你一窥图计算执行计划》[1]），并将任务调度给Woker执行。

Worker：具体分布式地执行任务的单元，接收到Coordinator发送的Pipeline，执行具体的计算和查询逻辑。

Meta Service: 服务注册管理，Coordinator启动后，会将服务的地址和端口向MetaService进行注册，Client提交查询时从MetaService获取Coordinator的服务地址，进行连接。目前支持http和rpc两种方式。

组件间执行流程如下：

OLAP流程

操作指南

1. 定义图模型
   
以下图为例，图中有2种点person和software，以及2种边knows和creates。


图模型

图模型定义可参考《TuGraph Analytics图建模研发：为图计算业务提速增效》[2]，图定义语法为：

CREATE GRAPH dy_modern (

    Vertex person (
    
      id bigint ID,
      
      name varchar,
      
      age int
      
    ),
    
    Vertex software (
    
      id bigint ID,
      
      name varchar,
      
      lang varchar
      
    ),
    
    Edge knows (
    
      srcId bigint SOURCE ID,

      targetId bigint DESTINATION ID,

      weight int
      
    ),
    
    Edge creates (
    
      srcId bigint SOURCE ID,
      
      targetId bigint DESTINATION ID,
      
      weight int
      
    )
    
) WITH (

    storeType='rocksdb',
    
    shardCount = 2
    
);

2. 准备图数据
   
创建“加工”类型图任务，发布生成图作业。

USE GRAPH dy_modern;

INSERT INTO dy_modern.person(id, name, age)

SELECT 1, 'jim', 20

UNION ALL

SELECT 2, 'kate', 22

UNION ALL

SELECT 3, 'tom', 24;

INSERT INTO dy_modern.software(id, name, lang)

SELECT 4, 'software1', 'java'

UNION ALL

SELECT 5, 'software2', 'java';

INSERT INTO dy_modern.knows

SELECT 1,2,2

UNION ALL

SELECT 1,3,3

UNION ALL

SELECT 3,2,3;

INSERT INTO dy_modern.creates

SELECT 2,4,6

UNION ALL

SELECT 3,5,8

UNION ALL

SELECT 3,4,8;

图作业需要的worker数为23，在作业界面将参数进行修改，之后提交作业运行。

Worker数配置

3. 创建查询服务
   
创建图查询服务, 任务类型选择“图查询”，目标图选择刚才创建的图。

创建查询服务

发布任务后，使用默认参数即可，提交作业。

4. 执行查询
   
图查询服务的作业变成RUNNING状态后，可在任务界面点击“查询”进入图查询界面

进入图查询

输入相应的gql查询语句，点击“执行”，即可得到查询结果。

执行图查询

5. 图可视化
   
点击某个点，可以查看点关联的具体信息和属性，以及关联的其他点边。

点边视图

除了可视化的方式，也可以json形式看到返回的结果。

JSON视图

至此，我们就成功使用TuGraph Analytics实现了图上的交互式查询！是不是超简单！快来试一试吧！

重磅发布！TuGraph开源高性能原生图存储系统CStore

CStore已在蚂蚁集团内部具备多年大规模生产化经验。目前，CStore系统已全部开源到TuGraph Analytics仓库（geaflow-cstore模块），未来我们将不断迭代升级，提升系统性能。也非常欢迎图计算和图存储的开发者一起参与进来，共同推进CStore的开源和演进。

CStore是一款专门为图分析场景而设计的原生图存储引擎，它采用了Rust语言编写，使用基于图的存储结构，针对图分析场景进行特定优化。CStore可以存储包含千亿级点和万亿级边的图数据，在蚂蚁集团内部的多场景使用中，已经积累了多年的经验，存储容量达到了PB级别。

图存储系统

在设计数据存储引擎时，需要考虑众多因素。例如，所应用的具体场景，是联机分析处理(OLAP)还是联机事务处理(OLTP)；再如，数据存储方式的选择，内存、本地硬盘或分布式存储；还有就是存储数据的格式，像是文档、JSON，宽表，对象，键值对等。图存储引擎是诸多数据存储引擎中的一种，它专门用于图数据的存储。从DB-Engines网站的统计来看，Graph DBMS的热度从2014后一直领先其他类型数据库。

根据图数据存储方式的不同，现有的图存储引擎主要分为下面几大类：

1. 基于链表，例如Neo4j。

2. 基于Hash+链表，例如ArangoDB。

3. 基于Key-Value键值，例如Titan/JanusGraph/HugeGraph。

4. 基于传统关系型数据库，例如AgensGraph。

CStore基于Array+链表的方式存储数据，支持实时读写能力。为了提供更好的图分析能力，CStore把图元数据和属性分离存储，单独对元数据构建多级索引。CStore在存储方式和索引设计上都充分考虑到图语义，有效地结合图数据访问的特点，充分利用计算机硬件的优势，在蚂蚁内部多个场景中得到验证，取得很好的业务效果。接下来从建模、存储、索引三个方面来介绍CStore作为原生图存储引擎的特点。

原生图建模

图形数据非常适用于描述复杂的网络关系。与其他存储格式相比较，图形数据具有自身的独特之处。举例来说，在 NoSQL 数据库中，Key-Value 数据库只包含 Key 和 Value 这两种元素，用户更容易理解和操作，并且存储引擎的设计也可以遵循一定的规则。另外，对于 DBMS（数据库管理系统）来说，实体关系模型必须按照一定的规范来进行设计，通常情况下，要求业务表的设计至少要符合第三范式，以此来消除数据冗余现象。CStore基于属性图来建模，属性图是由点，边，以及点和边上的属性组成的图，点数据中记录了起始点ID和图元数据（标签，时间戳），边数据记录了起始点ID和图元数据（终点ID，方向，标签，时间戳），每个起始点ID唯一确定一组点边数据。

点边数据进入到CStore之后，把点边起始点ID转化为从0开始的ID，这个过程称之为ID化。由于用户定义的起始点ID格式不固定，蚂蚁内部使用的起始点通常大于20bytes，转化为4个bytes ID后，可以极大减少索引内存消耗；另外规范化ID之后，CStore使用array来做主键索引，array的index为ID，查询效率为O(1)。


ID字典

ID化之后，点边数据被序列化为PrimaryKey和GraphData，GraphData由SecondKey和Property组成。PrimaryKey记录点边ID，SecondKey记录图元数据，Property记录点边属性数据。

SecondKey编码格式如下，target id大小不固定，其他部分消耗20 bytes。write ts字段记录数据写入时间，被用于数据淘汰，sequence id记录了数据写入顺序，transaction和compaction都会用到。graph info记录是否是点，是否是入边，是否包含属性，标签，其他meta, 点边时间属性等信息。


图数据编码

这样编码有两点好处：

1. 点边数据变成CStore内部具有固定格式的二进制数据，有效减少内存消耗，方便后续数据写入磁盘；

2. 图元数据被压缩到8字节，这样可以把更多的图信息放到索引侧，能够提前在索引侧过滤掉用户不需要的数据。

许多图形数据库会选择将点边数据编码为键值对的形式存储。相比之下，CStore是一款针对图分析场景而设计的存储引擎，目的是提供更快的分析能力。为了实现这一点，CStore会对经过编码后的图形信息建立多级索引。在下面的内容里，我们将讨论如何存储数据以及构建多级索引。

友好图存储

蚂蚁内部图数据的访问有以下几个特点。

1. 通常相同起始点ID的点边数据大概率一起被访问；

2. 相同类型, 时间的数据通常被一起访问；

3. 用户点边数据的属性非常大，而点边元数据信息为固定结构且数据量不大；

4. 点数据查询频率大于边数据。

CStore在设计存储结构的时候，结合上述特点，对特定场景进行优化。针对第一个特点，CStore尽可能的把具有相同的起始点ID的点边数据存储到物理上连续的存储空间。点边数据被序列化之后，数据首先被写入GraphData Segment，它为内存数据结构，使用跳表来组成数据，维护了起始点ID到GraphData List的结构。GraphData Segment主要有两个功能：

1. 数据聚合。把相同的src_id的点边数据聚合到一起存储。

2. 排序。按照用户指定顺序把相同src_id的点边数据排序，用于构建二级索引。


GraphData Segment数据结构

GraphData Segment到达阈值之后被刷入到磁盘，数据被刷入到磁盘之前，把具有相同类型，写入时间（通常以天为单位）的数据作为一个分区写入到连续的物理存储空间，每个分区之间数据互相独立。


GraphData Segment分区

再者根据点边数据属性大而元数据固定的特点，CStore采用了属性分离的架构，把图属性和图元数据分离存储到不同的物理存储空间。一方面对于图元数据单独加索引能够加速图查询性能，另外一方面属性分离也能够减少数据compaction的读写放大开销。

另外CStore还支持用户配置点边数据存储到不同的存储介质，点边数据在物理上隔离。点边数据的访问频率在不同的场景有所不同，分离存储能够进一步优化数据访问性能，提前过滤掉不需要的数据，减少数据读放大。

高效图索引

CStore具有高效的图数据检索能力，支持多维度索引，不仅提供针对图元数据的分区索引，主键索引和二级索引，还支持针对点边属性的全文索引，可以根据指定的条件快速定位和返回数据。


图索引架构

GraphData Segment写入到磁盘之前，把相同起始点ID的GraphData根据用户指定的顺序排序，然后把排序好的SecondKey构建索引，把排序好的Property直接写入到属性文件（记为vs文件）。

CStore的分区索引对应到磁盘中某个目录，不同的分区数据落在不同的目录。主键索引是稀疏索引，默认索引间隔为2048，目前仅支持以起始点ID构建主键，主键索引常驻内存。通过主键索引，可以快速定位到属性存储的文件id和offset。


主键索引

二级索引包括min max索引，bloom filter索引等，用于快速判断某个稀疏索引块是否包含某个主键。min max索引记录某个索引块的上界和下界，bloom filter记录索引块的主键。

对于热点数据，二级索引做了特殊支持，比如一个起始点ID对应了10亿条边，如果没有索引，需要把满足min max和bloom filter索引的所有边读到内存过滤一次，返回用户指定的数据。为了加速这种场景，CStore对点边类型，指向，写入时间加了二级索引。比如用户只想要读取label为student，dt为2023-10-1数据，通过二级索引，首先过滤掉所有label非student的数据，然后过滤掉dt非2023-10-1的数据，可以极大减少数据读开销，提升用户查询性能。

二级索引

构建完索引，数据写入到索引文件（记为is文件），is文件按照LSM-Tree结构组织，key为主键索引中的index，value为索引block压缩后的数据。is文件内部对于key有序，第0层单个文件内部对于key有序，其他层文件间对于key有序。

并发Compaction

为了加快 LSM-Tree compact的性能，CStore 支持单层的多线程compact。在示意图中，每一个单独的compactor都代表一个compact工作线程。不同的compactor线程和compact清理线程（负责清理被compact的文件)将会并行执行。

以下是单个compactor内部运作过程的示意图，其中包括四个关键步骤：

1. 计算各层次得分并选择其中最高的；

2. 收集需要compact的文件；

3. 将上述收集到的文件进行compact并生成新的文件；

4. 最后一步是将新的文件更新至 LSM-Tree及内存索引中。

Compaction管理

编译构建

单独编译CStore代码需要提前准备Rust和C++工具链。

# install rust.

curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# install nightly toolchain.

rustup update && rustup toolchain install nightly && rustc --version

# install other dependencies. 

yum install make gcc gcc-c++ protobuf-devel protobuf clang

下载TuGraph Analytics代码后，执行以下命令编译CStore源代码：

git clone https://github.com/TuGraph-family/tugraph-analytics.git

cd tugraph-analytics/geaflow-cstore && make build

执行make help可以查看makefile提供的功能，当前版本的功能和对应描述如下：

geaflow-cstore 0.1.0

Usage: make <target>

Targets:

  build-dev                       Build the geaflow-cstore with dev version. Both <make build> and <make build-dev> work.
  
  build-release                   Build the geaflow-cstore with release version.
  
  fmt                             Format the code by rustfmt.toml.
  
  clippy                          Check statically code with clippy.
  
  test-all                        Run all integration tests and unit tests, include ignored tests.
  
  test                            Execute all the unit tests.
  
  test-fn                         Use "make test-fn mod=<func_name>" to specify which function of test to run.
  
  test-in                         Use "make test-in mod=<func_name>" to specify which target of integration tests to run.
  
  bench-all                       Run all benches.
  
  bench                           Use "make bench mod=<mod_name>" to specify which bench of mod to run.
  
  doc                             Generate the document of geaflow-cstore and open it in html.
  
  all                             Execute code style and static checks, release version compilation and tests in sequence.
  
  update                          Update all the dependences to the newest version, include rust analyzer.
  
  gen                             Generate the code described by proto.
  
  features                        Use "make features mod=<id>" {0->[default] 1->[hdfs]} to set the env in ~/.cstore_buildrc.
  
  clean                           Clean up the cargo cache.
  
  version                         Show the version of geaflow-cstore.
  
  help                            List optional commands.
  
未来规划

CStore已在蚂蚁集团内部具备多年大规模生产化经验，未来我们将不断迭代升级，提升系统性能。然而，在一些特定的情况下，我们认识到CStore还有进步的空间。在未来一至两年内，我们计划更加深入地探索优化图分析场景，支持图数据列式存储，图融合以及物化视图能力。同时，为了提高用户的体验度，我们也将引入remote compactor功能，以解决因compact而导致的资源竞争问题。

目前，CStore系统已全部开源到TuGraph Analytics仓库（geaflow-cstore模块），当前推送的代码涵盖了CStore的核心功能特性，仍有大量的增强功能还在整合中。TuGraph团队将持续投入图存储系统CStore的建设工作，也非常欢迎图计算和图存储的开发者一起参与进来，共同推进CStore的开源和演进。

总结

当下TuGraph Analytics项目已经内置了CStore JNI的依赖，你可以在高阶API任务中使用并体验CStore静态图存储系统，CStore动态图能力正在建设中，敬请期待。同时我们也非常欢迎图存储系统/Rust爱好者加入到CStore开源项目的建设中来。

